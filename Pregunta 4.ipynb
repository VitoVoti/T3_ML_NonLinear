{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Tarea 3 Máquinas de Aprendizaje</center>\n",
    "\n",
    "<center>\n",
    "Patricio Horth M.<br>\n",
    "Víctor Zúñiga M.<br>\n",
    "\n",
    "22 de Diciembre de 2017\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Imágenes de Lenguaje de Señas\n",
    "\n",
    "## a) Ingreso y análisis descriptivo de los datos\n",
    "\n",
    "\"The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2....pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y training\n",
      "count    21964.000000\n",
      "mean        12.340056\n",
      "std          7.296453\n",
      "min          0.000000\n",
      "25%          6.000000\n",
      "50%         13.000000\n",
      "75%         19.000000\n",
      "max         24.000000\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Y validation\n",
      "count    5491.000000\n",
      "mean       12.233837\n",
      "std         7.251879\n",
      "min         0.000000\n",
      "25%         6.000000\n",
      "50%        13.000000\n",
      "75%        18.000000\n",
      "max        24.000000\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Y testing\n",
      "count    7172.000000\n",
      "mean       11.247351\n",
      "std         7.446712\n",
      "min         0.000000\n",
      "25%         4.000000\n",
      "50%        11.000000\n",
      "75%        18.000000\n",
      "max        24.000000\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv('sign_mnist_train.csv')\n",
    "    test = pd.read_csv('sign_mnist_test.csv')\n",
    "    y_tr = train['label']\n",
    "    x_tr = train.iloc[:,1:]\n",
    "    #print(\"Y training original\")\n",
    "    #print(y_tr.describe())\n",
    "    y_t = test['label']\n",
    "    x_t = test.iloc[:,1:]\n",
    "    #you need to add Xval: x_v,y_v\n",
    "    x_tr_two, x_v, y_tr_two, y_v = train_test_split(x_tr, y_tr, test_size=0.2, random_state=2)\n",
    "    print(\"Y training\")\n",
    "    print(y_tr_two.describe())\n",
    "    print(\"\\nY validation\")\n",
    "    print(y_v.describe())\n",
    "    print(\"\\nY testing\")\n",
    "    print(y_t.describe())\n",
    "    \n",
    "    \n",
    "    return(x_tr_two,x_v,x_t,y_tr_two,y_v,y_t)\n",
    "\n",
    "\n",
    "x_tr, x_v, x_t, y_tr, y_v , y_t= load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAEICAYAAAD82A0rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJA2A4U6SJEmSBsBwJ0mSJEkD8P8BE5TErijbEgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbb60d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import hist, show\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4\n",
    "\n",
    "plt.title(\"Histograma de y de validacion\")\n",
    "plt.hist(y_tr,bins=100)\n",
    "plt.xticks(range(0,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAEICAYAAAAeBBZSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSBsIAJ0mSJEkDYYCTJEmSpIH4Tw/bVRAAPqehAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc048a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import hist, show\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4\n",
    "\n",
    "plt.title(\"Histograma de y de validacion\")\n",
    "plt.hist(y_v,bins=100)\n",
    "plt.xticks(range(0,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAEICAYAAAAeBBZSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc4b1f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import hist, show\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4\n",
    "\n",
    "plt.title(\"Histograma de y de validacion\")\n",
    "plt.hist(y_t,bins=100)\n",
    "plt.xticks(range(0,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21964.000000\n",
       "mean        12.340056\n",
       "std          7.296453\n",
       "min          0.000000\n",
       "25%          6.000000\n",
       "50%         13.000000\n",
       "75%         19.000000\n",
       "max         24.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>145.315425</td>\n",
       "      <td>148.399973</td>\n",
       "      <td>151.177108</td>\n",
       "      <td>153.510199</td>\n",
       "      <td>156.160080</td>\n",
       "      <td>158.338326</td>\n",
       "      <td>160.400474</td>\n",
       "      <td>162.269760</td>\n",
       "      <td>163.882398</td>\n",
       "      <td>165.455427</td>\n",
       "      <td>...</td>\n",
       "      <td>141.187944</td>\n",
       "      <td>147.628938</td>\n",
       "      <td>153.461027</td>\n",
       "      <td>159.227008</td>\n",
       "      <td>162.075487</td>\n",
       "      <td>162.888363</td>\n",
       "      <td>163.107949</td>\n",
       "      <td>162.155983</td>\n",
       "      <td>161.246085</td>\n",
       "      <td>159.896285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41.209421</td>\n",
       "      <td>39.802453</td>\n",
       "      <td>38.836225</td>\n",
       "      <td>38.290959</td>\n",
       "      <td>36.869741</td>\n",
       "      <td>35.931102</td>\n",
       "      <td>34.858420</td>\n",
       "      <td>33.494483</td>\n",
       "      <td>32.495129</td>\n",
       "      <td>31.144656</td>\n",
       "      <td>...</td>\n",
       "      <td>63.725485</td>\n",
       "      <td>65.431314</td>\n",
       "      <td>64.418657</td>\n",
       "      <td>63.628838</td>\n",
       "      <td>63.666644</td>\n",
       "      <td>63.284134</td>\n",
       "      <td>63.324200</td>\n",
       "      <td>63.187234</td>\n",
       "      <td>63.521965</td>\n",
       "      <td>64.340061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>120.750000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>126.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>174.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pixel1        pixel2        pixel3        pixel4        pixel5  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean     145.315425    148.399973    151.177108    153.510199    156.160080   \n",
       "std       41.209421     39.802453     38.836225     38.290959     36.869741   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      121.000000    126.000000    130.000000    133.000000    137.000000   \n",
       "50%      150.000000    153.000000    155.000000    158.000000    160.000000   \n",
       "75%      174.000000    176.000000    178.000000    179.000000    181.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "             pixel6        pixel7        pixel8        pixel9       pixel10  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean     158.338326    160.400474    162.269760    163.882398    165.455427   \n",
       "std       35.931102     34.858420     33.494483     32.495129     31.144656   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      140.000000    142.000000    145.000000    146.000000    148.000000   \n",
       "50%      162.000000    163.000000    165.000000    166.000000    167.000000   \n",
       "75%      182.000000    183.000000    184.000000    185.000000    185.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           ...           pixel775      pixel776      pixel777      pixel778  \\\n",
       "count      ...       21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       ...         141.187944    147.628938    153.461027    159.227008   \n",
       "std        ...          63.725485     65.431314     64.418657     63.628838   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...          92.000000     96.000000    103.000000    112.000000   \n",
       "50%        ...         144.000000    163.000000    173.000000    180.000000   \n",
       "75%        ...         196.000000    202.000000    205.000000    207.000000   \n",
       "max        ...         255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean     162.075487    162.888363    163.107949    162.155983    161.246085   \n",
       "std       63.666644     63.284134     63.324200     63.187234     63.521965   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      120.750000    125.000000    128.000000    128.000000    128.000000   \n",
       "50%      183.000000    184.000000    184.000000    183.000000    182.000000   \n",
       "75%      208.000000    207.000000    207.000000    206.000000    205.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel784  \n",
       "count  21964.000000  \n",
       "mean     159.896285  \n",
       "std       64.340061  \n",
       "min        0.000000  \n",
       "25%      126.000000  \n",
       "50%      182.000000  \n",
       "75%      204.000000  \n",
       "max      255.000000  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15521</th>\n",
       "      <td>178</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>183</td>\n",
       "      <td>186</td>\n",
       "      <td>186</td>\n",
       "      <td>188</td>\n",
       "      <td>190</td>\n",
       "      <td>190</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>209</td>\n",
       "      <td>208</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>206</td>\n",
       "      <td>206</td>\n",
       "      <td>205</td>\n",
       "      <td>204</td>\n",
       "      <td>202</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>148</td>\n",
       "      <td>150</td>\n",
       "      <td>152</td>\n",
       "      <td>153</td>\n",
       "      <td>154</td>\n",
       "      <td>155</td>\n",
       "      <td>154</td>\n",
       "      <td>154</td>\n",
       "      <td>155</td>\n",
       "      <td>155</td>\n",
       "      <td>...</td>\n",
       "      <td>185</td>\n",
       "      <td>186</td>\n",
       "      <td>185</td>\n",
       "      <td>184</td>\n",
       "      <td>183</td>\n",
       "      <td>181</td>\n",
       "      <td>179</td>\n",
       "      <td>175</td>\n",
       "      <td>178</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26713</th>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "      <td>197</td>\n",
       "      <td>199</td>\n",
       "      <td>200</td>\n",
       "      <td>199</td>\n",
       "      <td>...</td>\n",
       "      <td>124</td>\n",
       "      <td>151</td>\n",
       "      <td>160</td>\n",
       "      <td>163</td>\n",
       "      <td>167</td>\n",
       "      <td>176</td>\n",
       "      <td>179</td>\n",
       "      <td>174</td>\n",
       "      <td>181</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23652</th>\n",
       "      <td>176</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>179</td>\n",
       "      <td>179</td>\n",
       "      <td>178</td>\n",
       "      <td>179</td>\n",
       "      <td>177</td>\n",
       "      <td>177</td>\n",
       "      <td>178</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13417</th>\n",
       "      <td>96</td>\n",
       "      <td>100</td>\n",
       "      <td>104</td>\n",
       "      <td>110</td>\n",
       "      <td>116</td>\n",
       "      <td>121</td>\n",
       "      <td>129</td>\n",
       "      <td>135</td>\n",
       "      <td>141</td>\n",
       "      <td>145</td>\n",
       "      <td>...</td>\n",
       "      <td>218</td>\n",
       "      <td>211</td>\n",
       "      <td>215</td>\n",
       "      <td>219</td>\n",
       "      <td>220</td>\n",
       "      <td>221</td>\n",
       "      <td>221</td>\n",
       "      <td>223</td>\n",
       "      <td>224</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "15521     178     181     182     183     186     186     188     190     190   \n",
       "9897      148     150     152     153     154     155     154     154     155   \n",
       "26713     197     197     198     199     198     198     197     199     200   \n",
       "23652     176     177     177     179     179     178     179     177     177   \n",
       "13417      96     100     104     110     116     121     129     135     141   \n",
       "\n",
       "       pixel10    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "15521      189    ...          209       208       207       207       206   \n",
       "9897       155    ...          185       186       185       184       183   \n",
       "26713      199    ...          124       151       160       163       167   \n",
       "23652      178    ...            0         0         0         0         0   \n",
       "13417      145    ...          218       211       215       219       220   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "15521       206       205       204       202       201  \n",
       "9897        181       179       175       178       140  \n",
       "26713       176       179       174       181       189  \n",
       "23652         0        16         0         0         0  \n",
       "13417       221       221       223       224       223  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15521    13\n",
       "9897      7\n",
       "26713     7\n",
       "23652    14\n",
       "13417     5\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Preprocesamiento de los datos\n",
    "\n",
    "A continuacion ...\n",
    "\n",
    "Siendo que los datos son pixeles en escala de grises con valores de 1 a 255, se dividirá cada celda por 255 para que estén en un rango $[0,1]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = x_tr / 255\n",
    "X_val = x_v / 255 \n",
    "X_test = x_t / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.569864</td>\n",
       "      <td>0.581961</td>\n",
       "      <td>0.592851</td>\n",
       "      <td>0.602001</td>\n",
       "      <td>0.612392</td>\n",
       "      <td>0.620935</td>\n",
       "      <td>0.629021</td>\n",
       "      <td>0.636352</td>\n",
       "      <td>0.642676</td>\n",
       "      <td>0.648845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553678</td>\n",
       "      <td>0.578937</td>\n",
       "      <td>0.601808</td>\n",
       "      <td>0.624420</td>\n",
       "      <td>0.635590</td>\n",
       "      <td>0.638778</td>\n",
       "      <td>0.639639</td>\n",
       "      <td>0.635906</td>\n",
       "      <td>0.632338</td>\n",
       "      <td>0.627044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.161606</td>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.152299</td>\n",
       "      <td>0.150161</td>\n",
       "      <td>0.144587</td>\n",
       "      <td>0.140906</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>0.131351</td>\n",
       "      <td>0.127432</td>\n",
       "      <td>0.122136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.249904</td>\n",
       "      <td>0.256593</td>\n",
       "      <td>0.252622</td>\n",
       "      <td>0.249525</td>\n",
       "      <td>0.249673</td>\n",
       "      <td>0.248173</td>\n",
       "      <td>0.248330</td>\n",
       "      <td>0.247793</td>\n",
       "      <td>0.249106</td>\n",
       "      <td>0.252314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.521569</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.572549</td>\n",
       "      <td>0.580392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.473529</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.494118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>0.654902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.678431</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.713725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pixel1        pixel2        pixel3        pixel4        pixel5  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       0.569864      0.581961      0.592851      0.602001      0.612392   \n",
       "std        0.161606      0.156088      0.152299      0.150161      0.144587   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.474510      0.494118      0.509804      0.521569      0.537255   \n",
       "50%        0.588235      0.600000      0.607843      0.619608      0.627451   \n",
       "75%        0.682353      0.690196      0.698039      0.701961      0.709804   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             pixel6        pixel7        pixel8        pixel9       pixel10  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       0.620935      0.629021      0.636352      0.642676      0.648845   \n",
       "std        0.140906      0.136700      0.131351      0.127432      0.122136   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.549020      0.556863      0.568627      0.572549      0.580392   \n",
       "50%        0.635294      0.639216      0.647059      0.650980      0.654902   \n",
       "75%        0.713725      0.717647      0.721569      0.725490      0.725490   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...           pixel775      pixel776      pixel777      pixel778  \\\n",
       "count      ...       21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       ...           0.553678      0.578937      0.601808      0.624420   \n",
       "std        ...           0.249904      0.256593      0.252622      0.249525   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.360784      0.376471      0.403922      0.439216   \n",
       "50%        ...           0.564706      0.639216      0.678431      0.705882   \n",
       "75%        ...           0.768627      0.792157      0.803922      0.811765   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       0.635590      0.638778      0.639639      0.635906      0.632338   \n",
       "std        0.249673      0.248173      0.248330      0.247793      0.249106   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.473529      0.490196      0.501961      0.501961      0.501961   \n",
       "50%        0.717647      0.721569      0.721569      0.717647      0.713725   \n",
       "75%        0.815686      0.811765      0.811765      0.807843      0.803922   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pixel784  \n",
       "count  21964.000000  \n",
       "mean       0.627044  \n",
       "std        0.252314  \n",
       "min        0.000000  \n",
       "25%        0.494118  \n",
       "50%        0.713725  \n",
       "75%        0.800000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15521</th>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.737255</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.788235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9897</th>\n",
       "      <td>0.580392</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.596078</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.603922</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.603922</td>\n",
       "      <td>0.603922</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.549020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26713</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.780392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.654902</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.741176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23652</th>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13417</th>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.392157</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.454902</td>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.552941</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>...</td>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.827451</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.878431</td>\n",
       "      <td>0.874510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pixel1    pixel2    pixel3    pixel4    pixel5    pixel6    pixel7  \\\n",
       "15521  0.698039  0.709804  0.713725  0.717647  0.729412  0.729412  0.737255   \n",
       "9897   0.580392  0.588235  0.596078  0.600000  0.603922  0.607843  0.603922   \n",
       "26713  0.772549  0.772549  0.776471  0.780392  0.776471  0.776471  0.772549   \n",
       "23652  0.690196  0.694118  0.694118  0.701961  0.701961  0.698039  0.701961   \n",
       "13417  0.376471  0.392157  0.407843  0.431373  0.454902  0.474510  0.505882   \n",
       "\n",
       "         pixel8    pixel9   pixel10    ...     pixel775  pixel776  pixel777  \\\n",
       "15521  0.745098  0.745098  0.741176    ...     0.819608  0.815686  0.811765   \n",
       "9897   0.603922  0.607843  0.607843    ...     0.725490  0.729412  0.725490   \n",
       "26713  0.780392  0.784314  0.780392    ...     0.486275  0.592157  0.627451   \n",
       "23652  0.694118  0.694118  0.698039    ...     0.000000  0.000000  0.000000   \n",
       "13417  0.529412  0.552941  0.568627    ...     0.854902  0.827451  0.843137   \n",
       "\n",
       "       pixel778  pixel779  pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "15521  0.811765  0.807843  0.807843  0.803922  0.800000  0.792157  0.788235  \n",
       "9897   0.721569  0.717647  0.709804  0.701961  0.686275  0.698039  0.549020  \n",
       "26713  0.639216  0.654902  0.690196  0.701961  0.682353  0.709804  0.741176  \n",
       "23652  0.000000  0.000000  0.000000  0.062745  0.000000  0.000000  0.000000  \n",
       "13417  0.858824  0.862745  0.866667  0.866667  0.874510  0.878431  0.874510  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAEICAYAAAAeBBZSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc841ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import hist, show\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4\n",
    "\n",
    "plt.title(\"Histograma de valores de pixel1 en conj. de entrenamiento\")\n",
    "plt.hist(X_train['pixel1'],bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "df_train_std = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "df_val_std = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "df_test_std = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.918354e-16</td>\n",
       "      <td>-4.420395e-16</td>\n",
       "      <td>3.531115e-17</td>\n",
       "      <td>7.673790e-17</td>\n",
       "      <td>-5.281634e-17</td>\n",
       "      <td>6.256706e-16</td>\n",
       "      <td>-3.544434e-16</td>\n",
       "      <td>5.630449e-16</td>\n",
       "      <td>-7.550391e-17</td>\n",
       "      <td>6.943140e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.793570e-17</td>\n",
       "      <td>-1.454148e-16</td>\n",
       "      <td>1.440803e-16</td>\n",
       "      <td>7.961215e-17</td>\n",
       "      <td>-3.121656e-16</td>\n",
       "      <td>1.447475e-16</td>\n",
       "      <td>-1.083382e-16</td>\n",
       "      <td>-1.355631e-16</td>\n",
       "      <td>-2.373049e-16</td>\n",
       "      <td>8.334255e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.526347e+00</td>\n",
       "      <td>-3.728498e+00</td>\n",
       "      <td>-3.892771e+00</td>\n",
       "      <td>-4.009137e+00</td>\n",
       "      <td>-4.235550e+00</td>\n",
       "      <td>-4.406821e+00</td>\n",
       "      <td>-4.601589e+00</td>\n",
       "      <td>-4.844782e+00</td>\n",
       "      <td>-5.043406e+00</td>\n",
       "      <td>-5.312603e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.215615e+00</td>\n",
       "      <td>-2.256294e+00</td>\n",
       "      <td>-2.382299e+00</td>\n",
       "      <td>-2.502492e+00</td>\n",
       "      <td>-2.545747e+00</td>\n",
       "      <td>-2.573980e+00</td>\n",
       "      <td>-2.575819e+00</td>\n",
       "      <td>-2.566336e+00</td>\n",
       "      <td>-2.538488e+00</td>\n",
       "      <td>-2.485231e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.900587e-01</td>\n",
       "      <td>-5.627915e-01</td>\n",
       "      <td>-5.453051e-01</td>\n",
       "      <td>-5.356530e-01</td>\n",
       "      <td>-5.196813e-01</td>\n",
       "      <td>-5.103863e-01</td>\n",
       "      <td>-5.278751e-01</td>\n",
       "      <td>-5.156119e-01</td>\n",
       "      <td>-5.503227e-01</td>\n",
       "      <td>-5.604758e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.718900e-01</td>\n",
       "      <td>-7.890735e-01</td>\n",
       "      <td>-7.833472e-01</td>\n",
       "      <td>-7.422434e-01</td>\n",
       "      <td>-6.491064e-01</td>\n",
       "      <td>-5.987160e-01</td>\n",
       "      <td>-5.544286e-01</td>\n",
       "      <td>-5.405643e-01</td>\n",
       "      <td>-5.233913e-01</td>\n",
       "      <td>-5.268422e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.136799e-01</td>\n",
       "      <td>1.155741e-01</td>\n",
       "      <td>9.843848e-02</td>\n",
       "      <td>1.172575e-01</td>\n",
       "      <td>1.041506e-01</td>\n",
       "      <td>1.019105e-01</td>\n",
       "      <td>7.457554e-02</td>\n",
       "      <td>8.151499e-02</td>\n",
       "      <td>6.516822e-02</td>\n",
       "      <td>4.959464e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>4.412866e-02</td>\n",
       "      <td>2.349244e-01</td>\n",
       "      <td>3.033192e-01</td>\n",
       "      <td>3.264788e-01</td>\n",
       "      <td>3.286649e-01</td>\n",
       "      <td>3.336084e-01</td>\n",
       "      <td>3.299296e-01</td>\n",
       "      <td>3.298845e-01</td>\n",
       "      <td>3.267277e-01</td>\n",
       "      <td>3.435530e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.960842e-01</td>\n",
       "      <td>6.934411e-01</td>\n",
       "      <td>6.906825e-01</td>\n",
       "      <td>6.657024e-01</td>\n",
       "      <td>6.737364e-01</td>\n",
       "      <td>6.585440e-01</td>\n",
       "      <td>6.483381e-01</td>\n",
       "      <td>6.487855e-01</td>\n",
       "      <td>6.498846e-01</td>\n",
       "      <td>6.275561e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.601473e-01</td>\n",
       "      <td>8.309828e-01</td>\n",
       "      <td>8.000810e-01</td>\n",
       "      <td>7.508243e-01</td>\n",
       "      <td>7.213441e-01</td>\n",
       "      <td>6.970569e-01</td>\n",
       "      <td>6.931481e-01</td>\n",
       "      <td>6.938904e-01</td>\n",
       "      <td>6.888155e-01</td>\n",
       "      <td>6.854939e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.661699e+00</td>\n",
       "      <td>2.678289e+00</td>\n",
       "      <td>2.673413e+00</td>\n",
       "      <td>2.650550e+00</td>\n",
       "      <td>2.680848e+00</td>\n",
       "      <td>2.690256e+00</td>\n",
       "      <td>2.713883e+00</td>\n",
       "      <td>2.768586e+00</td>\n",
       "      <td>2.804103e+00</td>\n",
       "      <td>2.875184e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.786015e+00</td>\n",
       "      <td>1.641011e+00</td>\n",
       "      <td>1.576271e+00</td>\n",
       "      <td>1.505216e+00</td>\n",
       "      <td>1.459581e+00</td>\n",
       "      <td>1.455558e+00</td>\n",
       "      <td>1.451169e+00</td>\n",
       "      <td>1.469381e+00</td>\n",
       "      <td>1.475963e+00</td>\n",
       "      <td>1.478175e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pixel1        pixel2        pixel3        pixel4        pixel5  \\\n",
       "count  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04   \n",
       "mean   2.918354e-16 -4.420395e-16  3.531115e-17  7.673790e-17 -5.281634e-17   \n",
       "std    1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00   \n",
       "min   -3.526347e+00 -3.728498e+00 -3.892771e+00 -4.009137e+00 -4.235550e+00   \n",
       "25%   -5.900587e-01 -5.627915e-01 -5.453051e-01 -5.356530e-01 -5.196813e-01   \n",
       "50%    1.136799e-01  1.155741e-01  9.843848e-02  1.172575e-01  1.041506e-01   \n",
       "75%    6.960842e-01  6.934411e-01  6.906825e-01  6.657024e-01  6.737364e-01   \n",
       "max    2.661699e+00  2.678289e+00  2.673413e+00  2.650550e+00  2.680848e+00   \n",
       "\n",
       "             pixel6        pixel7        pixel8        pixel9       pixel10  \\\n",
       "count  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04   \n",
       "mean   6.256706e-16 -3.544434e-16  5.630449e-16 -7.550391e-17  6.943140e-16   \n",
       "std    1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00   \n",
       "min   -4.406821e+00 -4.601589e+00 -4.844782e+00 -5.043406e+00 -5.312603e+00   \n",
       "25%   -5.103863e-01 -5.278751e-01 -5.156119e-01 -5.503227e-01 -5.604758e-01   \n",
       "50%    1.019105e-01  7.457554e-02  8.151499e-02  6.516822e-02  4.959464e-02   \n",
       "75%    6.585440e-01  6.483381e-01  6.487855e-01  6.498846e-01  6.275561e-01   \n",
       "max    2.690256e+00  2.713883e+00  2.768586e+00  2.804103e+00  2.875184e+00   \n",
       "\n",
       "           ...           pixel775      pixel776      pixel777      pixel778  \\\n",
       "count      ...       2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04   \n",
       "mean       ...      -6.793570e-17 -1.454148e-16  1.440803e-16  7.961215e-17   \n",
       "std        ...       1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00   \n",
       "min        ...      -2.215615e+00 -2.256294e+00 -2.382299e+00 -2.502492e+00   \n",
       "25%        ...      -7.718900e-01 -7.890735e-01 -7.833472e-01 -7.422434e-01   \n",
       "50%        ...       4.412866e-02  2.349244e-01  3.033192e-01  3.264788e-01   \n",
       "75%        ...       8.601473e-01  8.309828e-01  8.000810e-01  7.508243e-01   \n",
       "max        ...       1.786015e+00  1.641011e+00  1.576271e+00  1.505216e+00   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04   \n",
       "mean  -3.121656e-16  1.447475e-16 -1.083382e-16 -1.355631e-16 -2.373049e-16   \n",
       "std    1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00   \n",
       "min   -2.545747e+00 -2.573980e+00 -2.575819e+00 -2.566336e+00 -2.538488e+00   \n",
       "25%   -6.491064e-01 -5.987160e-01 -5.544286e-01 -5.405643e-01 -5.233913e-01   \n",
       "50%    3.286649e-01  3.336084e-01  3.299296e-01  3.298845e-01  3.267277e-01   \n",
       "75%    7.213441e-01  6.970569e-01  6.931481e-01  6.938904e-01  6.888155e-01   \n",
       "max    1.459581e+00  1.455558e+00  1.451169e+00  1.469381e+00  1.475963e+00   \n",
       "\n",
       "           pixel784  \n",
       "count  2.196400e+04  \n",
       "mean   8.334255e-17  \n",
       "std    1.000023e+00  \n",
       "min   -2.485231e+00  \n",
       "25%   -5.268422e-01  \n",
       "50%    3.435530e-01  \n",
       "75%    6.854939e-01  \n",
       "max    1.478175e+00  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_std.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.79315161,  0.06514617,  1.25422173, ...,  0.11367986,\n",
       "        0.30781465,  0.2350141 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_std['pixel1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAEICAYAAAAeBBZSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJnCSJEmS1BP/H+JYOU+hLyL9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xc793d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Histograma de valores de pixel1 en conj. de entrenamiento\")\n",
    "plt.hist(df_train_std['pixel1'],bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Boti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Boti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Boti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Boti\\Anaconda3\\lib\\site-packages\\keras\\models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 15.4297 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 2/100\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 3/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 4/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4877 - acc: 0.03 - 2s 85us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 5/100\n",
      "21964/21964 [==============================] - 2s 73us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 6/100\n",
      "21964/21964 [==============================] - 3s 114us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 7/100\n",
      "21964/21964 [==============================] - 2s 104us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 8/100\n",
      "21964/21964 [==============================] - 2s 81us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 9/100\n",
      "21964/21964 [==============================] - 2s 86us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 10/100\n",
      "21964/21964 [==============================] - 3s 114us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 11/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4908 - acc: 0.03 - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 12/100\n",
      "21964/21964 [==============================] - 3s 115us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 13/100\n",
      "21964/21964 [==============================] - 2s 99us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 14/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4937 - acc: 0.03 - 2s 90us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 15/100\n",
      "21964/21964 [==============================] - 3s 135us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 16/100\n",
      "21964/21964 [==============================] - 2s 107us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 17/100\n",
      "21964/21964 [==============================] - 3s 119us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 18/100\n",
      "21964/21964 [==============================] - 2s 98us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 19/100\n",
      "21964/21964 [==============================] - 2s 105us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 20/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 21/100\n",
      "21964/21964 [==============================] - 2s 93us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 22/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 23/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 24/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 25/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 26/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 27/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 28/100\n",
      "21964/21964 [==============================] - 1s 44us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 29/100\n",
      "21964/21964 [==============================] - 2s 104us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437oss:\n",
      "Epoch 30/100\n",
      "21964/21964 [==============================] - 2s 99us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 31/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 32/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 33/100\n",
      "21964/21964 [==============================] - 3s 129us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 34/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 35/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 36/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 37/100\n",
      "21964/21964 [==============================] - 2s 80us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 38/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 39/100\n",
      "21964/21964 [==============================] - 2s 103us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 40/100\n",
      "21964/21964 [==============================] - 2s 112us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 41/100\n",
      "21964/21964 [==============================] - 2s 111us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 42/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 43/100\n",
      "21964/21964 [==============================] - 3s 116us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 44/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 45/100\n",
      "21964/21964 [==============================] - 2s 113us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 46/100\n",
      "21964/21964 [==============================] - 3s 133us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 47/100\n",
      "21964/21964 [==============================] - 2s 112us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 48/100\n",
      "21964/21964 [==============================] - 4s 165us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 49/100\n",
      "21964/21964 [==============================] - 3s 140us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 50/100\n",
      "21964/21964 [==============================] - 3s 128us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 51/100\n",
      "21964/21964 [==============================] - 3s 115us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 52/100\n",
      "21964/21964 [==============================] - 3s 143us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 53/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 54/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 55/100\n",
      "21964/21964 [==============================] - 2s 70us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 56/100\n",
      "21964/21964 [==============================] - 3s 116us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 57/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "21964/21964 [==============================] - 2s 68us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 59/100\n",
      "21964/21964 [==============================] - 2s 110us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 60/100\n",
      "21964/21964 [==============================] - 3s 119us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 61/100\n",
      "21964/21964 [==============================] - 2s 86us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 62/100\n",
      "21964/21964 [==============================] - 3s 118us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 63/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 64/100\n",
      "21964/21964 [==============================] - 2s 74us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 65/100\n",
      "21964/21964 [==============================] - 2s 77us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 66/100\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 67/100\n",
      "21964/21964 [==============================] - 3s 130us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 68/100\n",
      "21964/21964 [==============================] - 3s 127us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 69/100\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 70/100\n",
      "21964/21964 [==============================] - 3s 139us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 71/100\n",
      "21964/21964 [==============================] - 2s 82us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 72/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 73/100\n",
      "21964/21964 [==============================] - 2s 73us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 74/100\n",
      "21964/21964 [==============================] - 2s 110us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 75/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 76/100\n",
      "21964/21964 [==============================] - 2s 88us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 77/100\n",
      "21964/21964 [==============================] - 3s 133us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 78/100\n",
      "21964/21964 [==============================] - 2s 105us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 79/100\n",
      "21964/21964 [==============================] - 3s 133us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 80/100\n",
      "21964/21964 [==============================] - 2s 93us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 81/100\n",
      "21964/21964 [==============================] - 2s 112us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 82/100\n",
      "21964/21964 [==============================] - 3s 141us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.04375.49\n",
      "Epoch 83/100\n",
      "21964/21964 [==============================] - 1s 67us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 84/100\n",
      "21964/21964 [==============================] - 2s 74us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 85/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 86/100\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 87/100\n",
      "21964/21964 [==============================] - 3s 139us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 88/100\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 89/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 90/100\n",
      "21964/21964 [==============================] - 2s 86us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 91/100\n",
      "21964/21964 [==============================] - 3s 126us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 92/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 93/100\n",
      "21964/21964 [==============================] - 2s 113us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 94/100\n",
      "21964/21964 [==============================] - 2s 81us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 95/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 96/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4922 - acc: 0.0388 ETA: 0s - loss: 15.4927 - - 3s 119us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 97/100\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 98/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 99/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4892 - acc: 0.0390 ETA:  - 2s 79us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 100/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28abfe48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Modelo secuencial (feed forward)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1, validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/100\n",
      "21964/21964 [==============================] - 2s 91us/step - loss: 3.1924 - acc: 0.0982 - val_loss: 3.1207 - val_acc: 0.1018\n",
      "Epoch 2/100\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 2.8818 - acc: 0.1269 - val_loss: 2.6251 - val_acc: 0.1515\n",
      "Epoch 3/100\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 2.2955 - acc: 0.2632 - val_loss: 1.9061 - val_acc: 0.3661\n",
      "Epoch 4/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 1.6256 - acc: 0.4547 - val_loss: 1.4097 - val_acc: 0.5265\n",
      "Epoch 5/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 1.1973 - acc: 0.5930 - val_loss: 1.0383 - val_acc: 0.6431\n",
      "Epoch 6/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 0.8562 - acc: 0.7184 - val_loss: 0.7440 - val_acc: 0.7483loss: 0.8873 - ac\n",
      "Epoch 7/100\n",
      "21964/21964 [==============================] - 2s 111us/step - loss: 0.5947 - acc: 0.8117 - val_loss: 0.5115 - val_acc: 0.8510\n",
      "Epoch 8/100\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.4057 - acc: 0.8829 - val_loss: 0.3555 - val_acc: 0.9075\n",
      "Epoch 9/100\n",
      "21964/21964 [==============================] - 2s 73us/step - loss: 0.2702 - acc: 0.9358 - val_loss: 0.2247 - val_acc: 0.9556\n",
      "Epoch 10/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 0.1774 - acc: 0.9668 - val_loss: 0.1538 - val_acc: 0.9752\n",
      "Epoch 11/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 0.1195 - acc: 0.984 - 1s 49us/step - loss: 0.1183 - acc: 0.9842 - val_loss: 0.1067 - val_acc: 0.9854\n",
      "Epoch 12/100\n",
      "21964/21964 [==============================] - 2s 105us/step - loss: 0.0817 - acc: 0.9931 - val_loss: 0.0735 - val_acc: 0.9962\n",
      "Epoch 13/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 0.0574 - acc: 0.997 - 1s 49us/step - loss: 0.0574 - acc: 0.9970 - val_loss: 0.0570 - val_acc: 0.9969\n",
      "Epoch 14/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 0.0427 - acc: 0.999 - 2s 92us/step - loss: 0.0427 - acc: 0.9992 - val_loss: 0.0425 - val_acc: 0.9982\n",
      "Epoch 15/100\n",
      "21964/21964 [==============================] - 2s 92us/step - loss: 0.0333 - acc: 0.9994 - val_loss: 0.0338 - val_acc: 0.9996\n",
      "Epoch 16/100\n",
      "21964/21964 [==============================] - 2s 79us/step - loss: 0.0266 - acc: 0.9995 - val_loss: 0.0274 - val_acc: 0.9989\n",
      "Epoch 17/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 0.0220 - acc: 0.9996 - val_loss: 0.0229 - val_acc: 0.9996 1s - loss: 0.02 - ETA: 0s - loss: 0.0220 - acc: 0.\n",
      "Epoch 18/100\n",
      "21964/21964 [==============================] - 2s 110us/step - loss: 0.0184 - acc: 0.9999 - val_loss: 0.0198 - val_acc: 0.9995\n",
      "Epoch 19/100\n",
      "21964/21964 [==============================] - 2s 109us/step - loss: 0.0157 - acc: 0.9999 - val_loss: 0.0174 - val_acc: 0.9996\n",
      "Epoch 20/100\n",
      "21964/21964 [==============================] - 1s 66us/step - loss: 0.0137 - acc: 0.9999 - val_loss: 0.0153 - val_acc: 0.9995\n",
      "Epoch 21/100\n",
      "21964/21964 [==============================] - 2s 92us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 0.9995\n",
      "Epoch 22/100\n",
      "21964/21964 [==============================] - 2s 72us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0123 - val_acc: 0.9996\n",
      "Epoch 23/100\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0113 - val_acc: 0.9996\n",
      "Epoch 24/100\n",
      "21964/21964 [==============================] - 2s 104us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.0105 - val_acc: 0.9995c: 1.0\n",
      "Epoch 25/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0096 - val_acc: 0.9996\n",
      "Epoch 26/100\n",
      "21964/21964 [==============================] - 2s 87us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0088 - val_acc: 0.9998\n",
      "Epoch 27/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 0.9996\n",
      "Epoch 28/100\n",
      "21964/21964 [==============================] - 2s 98us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9996\n",
      "Epoch 29/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9998\n",
      "Epoch 30/100\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 0.9998\n",
      "Epoch 31/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 0.9998\n",
      "Epoch 32/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9998\n",
      "Epoch 33/100\n",
      "21964/21964 [==============================] - 2s 111us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9998\n",
      "Epoch 34/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9998\n",
      "Epoch 35/100\n",
      "21964/21964 [==============================] - 2s 101us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9998s: 0.0041 \n",
      "Epoch 36/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 0.9998\n",
      "Epoch 37/100\n",
      "21964/21964 [==============================] - 2s 75us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9998\n",
      "Epoch 38/100\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9998\n",
      "Epoch 39/100\n",
      "21964/21964 [==============================] - 2s 70us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9998\n",
      "Epoch 40/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9998\n",
      "Epoch 41/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9998\n",
      "Epoch 42/100\n",
      "21964/21964 [==============================] - 2s 79us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9998\n",
      "Epoch 43/100\n",
      "21964/21964 [==============================] - 3s 122us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9998\n",
      "Epoch 44/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9998\n",
      "Epoch 45/100\n",
      "21964/21964 [==============================] - 2s 105us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9998\n",
      "Epoch 46/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9998\n",
      "Epoch 47/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9998\n",
      "Epoch 48/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9998\n",
      "Epoch 49/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9998\n",
      "Epoch 50/100\n",
      "21964/21964 [==============================] - 2s 87us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 0.9998\n",
      "Epoch 51/100\n",
      "21964/21964 [==============================] - 2s 93us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9998\n",
      "Epoch 52/100\n",
      "21964/21964 [==============================] - 3s 154us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 0.9998\n",
      "Epoch 53/100\n",
      "21964/21964 [==============================] - 3s 139us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.999800 - ETA: 1s - loss: 0.0020 - acc: 1.0 - ETA: 1s - loss: 0.0020 - ETA: 0s - loss: 0.0020 - acc: 1.0\n",
      "Epoch 54/100\n",
      "21964/21964 [==============================] - 2s 83us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9998\n",
      "Epoch 55/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9998\n",
      "Epoch 56/100\n",
      "21964/21964 [==============================] - 2s 106us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9998\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21964/21964 [==============================] - 2s 94us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9998\n",
      "Epoch 58/100\n",
      "21964/21964 [==============================] - 3s 130us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9998 acc\n",
      "Epoch 59/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9998\n",
      "Epoch 60/100\n",
      "21964/21964 [==============================] - 3s 127us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9998\n",
      "Epoch 61/100\n",
      "21964/21964 [==============================] - 1s 67us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9998\n",
      "Epoch 62/100\n",
      "21964/21964 [==============================] - 3s 139us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9998\n",
      "Epoch 63/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9998\n",
      "Epoch 64/100\n",
      "21964/21964 [==============================] - 3s 135us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9998\n",
      "Epoch 65/100\n",
      "21964/21964 [==============================] - 3s 147us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9998\n",
      "Epoch 66/100\n",
      "21964/21964 [==============================] - 3s 122us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9998\n",
      "Epoch 67/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9998\n",
      "Epoch 68/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9998\n",
      "Epoch 69/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9998\n",
      "Epoch 70/100\n",
      "21964/21964 [==============================] - 2s 107us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9998s - loss: 0.0013 - ac\n",
      "Epoch 71/100\n",
      "21964/21964 [==============================] - 2s 92us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Epoch 72/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Epoch 73/100\n",
      "21964/21964 [==============================] - 3s 114us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Epoch 74/100\n",
      "21964/21964 [==============================] - 3s 120us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Epoch 75/100\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9998\n",
      "Epoch 76/100\n",
      "21964/21964 [==============================] - 2s 107us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9998\n",
      "Epoch 77/100\n",
      "21964/21964 [==============================] - 2s 114us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9998\n",
      "Epoch 78/100\n",
      "21964/21964 [==============================] - 2s 83us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9998\n",
      "Epoch 79/100\n",
      "21964/21964 [==============================] - 2s 94us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9998\n",
      "Epoch 80/100\n",
      "21964/21964 [==============================] - 2s 82us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9998\n",
      "Epoch 81/100\n",
      "21964/21964 [==============================] - 3s 127us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9998- loss: 0.0010 - acc: 1. - ETA: 1s - loss: 0.0011 - acc: 1.00 - \n",
      "Epoch 82/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 83/100\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 84/100\n",
      "21964/21964 [==============================] - 2s 73us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 85/100\n",
      "21964/21964 [==============================] - 2s 106us/step - loss: 9.9765e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 86/100\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 9.8002e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 87/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 9.6427e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 88/100\n",
      "21964/21964 [==============================] - 2s 113us/step - loss: 9.4980e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998 9.5428e-04 - acc: 1. - ETA: 0s - loss: 9.5467e-04 - acc: 1.0\n",
      "Epoch 89/100\n",
      "21964/21964 [==============================] - 3s 119us/step - loss: 9.3369e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 90/100\n",
      "21964/21964 [==============================] - 2s 79us/step - loss: 9.2000e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 91/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 9.0570e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998s - loss: 9.1405e-04 - - ETA: 0s - loss: 9.1211e-04 - \n",
      "Epoch 92/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 8.9155e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998\n",
      "Epoch 93/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 8.7889e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998\n",
      "Epoch 94/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 8.6558e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998\n",
      "Epoch 95/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 8.5277e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998\n",
      "Epoch 96/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 8.4051e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 97/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 8.2839e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 98/100\n",
      "21964/21964 [==============================] - 2s 80us/step - loss: 8.1706e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 99/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 8.0572e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 100/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 7.9464e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x35261f60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Modelo secuencial (feed forward)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(30, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(25, kernel_initializer='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(df_train_std.values, to_categorical(y_tr), epochs=100, batch_size=128, verbose=1, validation_data=(df_val_std.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (None, 25) but got array with shape (7172, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-eee308cb78d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[0;32m    920\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 922\u001b[1;33m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1679\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1680\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1681\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1682\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1413\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1414\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1415\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    152\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    155\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (None, 25) but got array with shape (7172, 1)"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(x_t.values, y_t.values, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Matriz de Confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Clasificación mediante SVM no lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "model = SVC(verbose=15, C=0.01)\n",
    "model.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model.predict(df_train_std)\n",
    "Y_pred_val = model.predict(df_val_std)\n",
    "Y_pred_test = model.predict(df_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.score(df_val_std, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Clasificación mediante Árbol de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  1.0\n",
      "Score Val:  0.875978874522\n",
      "Score Test:  0.458170663692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "\n",
    "model_tree_default = Tree(random_state=0) # Arbol por defecto\n",
    "model_tree_default.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model_tree_default.predict(df_train_std)\n",
    "Y_pred_val = model_tree_default.predict(df_val_std)\n",
    "Y_pred_test = model_tree_default.predict(df_test_std)\n",
    "\n",
    "print(\"Score Train: \", model_tree_default.score(df_train_std, y_tr))\n",
    "print(\"Score Val: \", model_tree_default.score(df_val_std, y_v))\n",
    "print(\"Score Test: \", model_tree_default.score(df_test_std, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tree_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 180 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:   37.8s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:   42.5s\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:   52.2s\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 234 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 257 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done 305 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 330 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done 360 out of 360 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores params: {'max_depth': 50, 'min_samples_leaf': 5, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_depth':[10, 20, 50, 200, None], 'min_samples_split': [5,10,15,20,50,100], 'min_samples_leaf': [5,10,15,20,50,100] }\n",
    "clf = GridSearchCV(Tree(random_state=0), parameters, verbose=10, n_jobs=4, cv=2)\n",
    "clf.fit(df_val_std, y_v)\n",
    "tree_model_cv = clf.best_estimator_\n",
    "print (\"Mejores params:\", clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 196 candidates, totalling 392 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 234 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done 257 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done 305 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done 330 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done 384 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done 392 out of 392 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores params: {'max_depth': 35, 'min_samples_leaf': 2, 'min_samples_split': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_depth':[20, 35, 50, 60], 'min_samples_split': [2,3,4,5,6,7,8], 'min_samples_leaf': [2,3,4,5,6,7,8] }\n",
    "clf = GridSearchCV(Tree(random_state=0), parameters, verbose=10, n_jobs=4, cv=2)\n",
    "clf.fit(df_val_std, y_v)\n",
    "tree_model_cv = clf.best_estimator_\n",
    "print (\"Mejores params:\", clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.931342196321\n",
      "Score Val:  0.83081405937\n",
      "Score Test:  0.442136084774\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV1\n",
    "# Datos sin preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=50, min_samples_leaf=5, min_samples_split=5)\n",
    "model.fit(x_tr, y_tr)\n",
    "Y_pred_train = model.predict(x_tr)\n",
    "Y_pred_val = model.predict(x_v)\n",
    "Y_pred_test = model.predict(x_t)\n",
    "\n",
    "print(\"Score Train: \", model.score(x_tr, y_tr))\n",
    "print(\"Score Val: \", model.score(x_v, y_v))\n",
    "print(\"Score Test: \", model.score(x_t, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.931342196321\n",
      "Score Val:  0.83044982699\n",
      "Score Test:  0.442554378137\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV1\n",
    "# Datos con preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=50, min_samples_leaf=5, min_samples_split=5)\n",
    "model.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model.predict(df_train_std)\n",
    "Y_pred_val = model.predict(df_val_std)\n",
    "Y_pred_test = model.predict(df_test_std)\n",
    "\n",
    "print(\"Score Train: \", model.score(df_train_std, y_tr))\n",
    "print(\"Score Val: \", model.score(df_val_std, y_v))\n",
    "print(\"Score Test: \", model.score(df_test_std, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.966991440539\n",
      "Score Val:  0.853760699326\n",
      "Score Test:  0.439068600112\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV2\n",
    "# Datos sin preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=35, min_samples_leaf=2, min_samples_split=6)\n",
    "model.fit(x_tr, y_tr)\n",
    "Y_pred_train = model.predict(x_tr)\n",
    "Y_pred_val = model.predict(x_v)\n",
    "Y_pred_test = model.predict(x_t)\n",
    "\n",
    "print(\"Score Train: \", model.score(x_tr, y_tr))\n",
    "print(\"Score Val: \", model.score(x_v, y_v))\n",
    "print(\"Score Test: \", model.score(x_t, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.966991440539\n",
      "Score Val:  0.852850118376\n",
      "Score Test:  0.439486893475\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV2\n",
    "# Datos con preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=35, min_samples_leaf=2, min_samples_split=6)\n",
    "model.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model.predict(df_train_std)\n",
    "Y_pred_val = model.predict(df_val_std)\n",
    "Y_pred_test = model.predict(df_test_std)\n",
    "\n",
    "print(\"Score Train: \", model.score(df_train_std, y_tr))\n",
    "print(\"Score Val: \", model.score(df_val_std, y_v))\n",
    "print(\"Score Test: \", model.score(df_test_std, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.917273720634\n",
      "Score Val:  0.820797668913\n",
      "Score Test:  0.436001115449\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV3\n",
    "# Datos sin preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=20, min_samples_leaf=5, min_samples_split=5)\n",
    "model.fit(x_tr, y_tr)\n",
    "Y_pred_train = model.predict(x_tr)\n",
    "Y_pred_val = model.predict(x_v)\n",
    "Y_pred_test = model.predict(x_t)\n",
    "\n",
    "print(\"Score Train: \", model.score(x_tr, y_tr))\n",
    "print(\"Score Val: \", model.score(x_v, y_v))\n",
    "print(\"Score Test: \", model.score(x_t, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.917273720634\n",
      "Score Val:  0.820069204152\n",
      "Score Test:  0.436279977691\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV3\n",
    "# Datos con preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=20, min_samples_leaf=5, min_samples_split=5)\n",
    "model.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model.predict(df_train_std)\n",
    "Y_pred_val = model.predict(df_val_std)\n",
    "Y_pred_test = model.predict(df_test_std)\n",
    "\n",
    "print(\"Score Train: \", model.score(df_train_std, y_tr))\n",
    "print(\"Score Val: \", model.score(df_val_std, y_v))\n",
    "print(\"Score Test: \", model.score(df_test_std, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
