{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Tarea 3 Máquinas de Aprendizaje</center>\n",
    "\n",
    "<center>\n",
    "Patricio Horth M.<br>\n",
    "Víctor Zúñiga M.<br>\n",
    "\n",
    "22 de Diciembre de 2017\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Imágenes de Lenguaje de Señas\n",
    "\n",
    "## a) Ingreso y análisis descriptivo de los datos\n",
    "\n",
    "\"The dataset format is patterned to match closely with the classic MNIST. Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z because of gesture motions). The training data (27,455 cases) and test data (7172 cases) are approximately half the size of the standard MNIST but otherwise similar with a header row of label, pixel1,pixel2....pixel784 which represent a single 28x28 pixel image with grayscale values between 0-255.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y training\n",
      "count    21964.000000\n",
      "mean        12.340056\n",
      "std          7.296453\n",
      "min          0.000000\n",
      "25%          6.000000\n",
      "50%         13.000000\n",
      "75%         19.000000\n",
      "max         24.000000\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Y validation\n",
      "count    5491.000000\n",
      "mean       12.233837\n",
      "std         7.251879\n",
      "min         0.000000\n",
      "25%         6.000000\n",
      "50%        13.000000\n",
      "75%        18.000000\n",
      "max        24.000000\n",
      "Name: label, dtype: float64\n",
      "\n",
      "Y testing\n",
      "count    7172.000000\n",
      "mean       11.247351\n",
      "std         7.446712\n",
      "min         0.000000\n",
      "25%         4.000000\n",
      "50%        11.000000\n",
      "75%        18.000000\n",
      "max        24.000000\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv('sign_mnist_train.csv')\n",
    "    test = pd.read_csv('sign_mnist_test.csv')\n",
    "    y_tr = train['label']\n",
    "    x_tr = train.iloc[:,1:]\n",
    "    #print(\"Y training original\")\n",
    "    #print(y_tr.describe())\n",
    "    y_t = test['label']\n",
    "    x_t = test.iloc[:,1:]\n",
    "    #you need to add Xval: x_v,y_v\n",
    "    x_tr_two, x_v, y_tr_two, y_v = train_test_split(x_tr, y_tr, test_size=0.2, random_state=2)\n",
    "    print(\"Y training\")\n",
    "    print(y_tr_two.describe())\n",
    "    print(\"\\nY validation\")\n",
    "    print(y_v.describe())\n",
    "    print(\"\\nY testing\")\n",
    "    print(y_t.describe())\n",
    "    \n",
    "    \n",
    "    return(x_tr_two,x_v,x_t,y_tr_two,y_v,y_t)\n",
    "\n",
    "\n",
    "x_tr, x_v, x_t, y_tr, y_v , y_t= load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3cAAAEICAYAAAD82A0rAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSJA2A4U6SJEmSBsBwJ0mSJEkD8P8BE5TErijbEgAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd1b4a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import hist, show\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4\n",
    "\n",
    "plt.title(\"Histograma de y de validacion\")\n",
    "plt.hist(y_tr,bins=100)\n",
    "plt.xticks(range(0,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAEICAYAAAAeBBZSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmSBsIAJ0mSJEkDYYCTJEmSpIH4Tw/bVRAAPqehAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd375390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import hist, show\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4\n",
    "\n",
    "plt.title(\"Histograma de y de validacion\")\n",
    "plt.hist(y_v,bins=100)\n",
    "plt.xticks(range(0,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAEICAYAAAAeBBZSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xd3bff60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import hist, show\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4\n",
    "\n",
    "plt.title(\"Histograma de y de validacion\")\n",
    "plt.hist(y_t,bins=100)\n",
    "plt.xticks(range(0,25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    21964.000000\n",
       "mean        12.340056\n",
       "std          7.296453\n",
       "min          0.000000\n",
       "25%          6.000000\n",
       "50%         13.000000\n",
       "75%         19.000000\n",
       "max         24.000000\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>145.547578</td>\n",
       "      <td>148.685622</td>\n",
       "      <td>151.430022</td>\n",
       "      <td>153.697004</td>\n",
       "      <td>156.361865</td>\n",
       "      <td>158.574759</td>\n",
       "      <td>160.617101</td>\n",
       "      <td>162.469951</td>\n",
       "      <td>164.084502</td>\n",
       "      <td>165.664451</td>\n",
       "      <td>...</td>\n",
       "      <td>141.003369</td>\n",
       "      <td>147.584320</td>\n",
       "      <td>153.317702</td>\n",
       "      <td>159.096658</td>\n",
       "      <td>162.115462</td>\n",
       "      <td>162.915544</td>\n",
       "      <td>163.136268</td>\n",
       "      <td>162.230969</td>\n",
       "      <td>161.431023</td>\n",
       "      <td>160.152659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>41.362641</td>\n",
       "      <td>39.831851</td>\n",
       "      <td>38.950932</td>\n",
       "      <td>38.560113</td>\n",
       "      <td>37.054798</td>\n",
       "      <td>36.079252</td>\n",
       "      <td>34.967191</td>\n",
       "      <td>33.625634</td>\n",
       "      <td>32.595575</td>\n",
       "      <td>31.203199</td>\n",
       "      <td>...</td>\n",
       "      <td>63.799788</td>\n",
       "      <td>65.468592</td>\n",
       "      <td>64.424339</td>\n",
       "      <td>63.719719</td>\n",
       "      <td>63.659376</td>\n",
       "      <td>63.399527</td>\n",
       "      <td>63.459460</td>\n",
       "      <td>63.238505</td>\n",
       "      <td>63.556360</td>\n",
       "      <td>64.296454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>121.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>128.000000</td>\n",
       "      <td>129.000000</td>\n",
       "      <td>127.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>174.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>180.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>183.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>208.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>207.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>204.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pixel1        pixel2        pixel3        pixel4        pixel5  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean     145.547578    148.685622    151.430022    153.697004    156.361865   \n",
       "std       41.362641     39.831851     38.950932     38.560113     37.054798   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      121.000000    126.000000    130.000000    133.000000    137.000000   \n",
       "50%      150.000000    153.000000    156.000000    158.000000    160.000000   \n",
       "75%      174.000000    176.000000    178.000000    180.000000    181.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "             pixel6        pixel7        pixel8        pixel9       pixel10  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean     158.574759    160.617101    162.469951    164.084502    165.664451   \n",
       "std       36.079252     34.967191     33.625634     32.595575     31.203199   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      140.000000    142.000000    145.000000    146.000000    148.000000   \n",
       "50%      162.000000    164.000000    165.000000    166.000000    168.000000   \n",
       "75%      182.000000    183.000000    184.000000    185.000000    186.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           ...           pixel775      pixel776      pixel777      pixel778  \\\n",
       "count      ...       21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       ...         141.003369    147.584320    153.317702    159.096658   \n",
       "std        ...          63.799788     65.468592     64.424339     63.719719   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...          92.000000     96.000000    103.000000    112.000000   \n",
       "50%        ...         143.000000    162.000000    172.000000    180.000000   \n",
       "75%        ...         196.000000    202.000000    205.000000    207.000000   \n",
       "max        ...         255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean     162.115462    162.915544    163.136268    162.230969    161.431023   \n",
       "std       63.659376     63.399527     63.459460     63.238505     63.556360   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%      120.000000    126.000000    129.000000    128.000000    129.000000   \n",
       "50%      183.000000    184.000000    184.000000    183.000000    182.000000   \n",
       "75%      208.000000    207.000000    207.000000    206.000000    205.000000   \n",
       "max      255.000000    255.000000    255.000000    255.000000    255.000000   \n",
       "\n",
       "           pixel784  \n",
       "count  21964.000000  \n",
       "mean     160.152659  \n",
       "std       64.296454  \n",
       "min        0.000000  \n",
       "25%      127.000000  \n",
       "50%      182.000000  \n",
       "75%      204.000000  \n",
       "max      255.000000  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17935</th>\n",
       "      <td>156</td>\n",
       "      <td>160</td>\n",
       "      <td>163</td>\n",
       "      <td>164</td>\n",
       "      <td>165</td>\n",
       "      <td>167</td>\n",
       "      <td>169</td>\n",
       "      <td>172</td>\n",
       "      <td>173</td>\n",
       "      <td>172</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>49</td>\n",
       "      <td>59</td>\n",
       "      <td>73</td>\n",
       "      <td>70</td>\n",
       "      <td>89</td>\n",
       "      <td>72</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8315</th>\n",
       "      <td>87</td>\n",
       "      <td>92</td>\n",
       "      <td>105</td>\n",
       "      <td>115</td>\n",
       "      <td>125</td>\n",
       "      <td>132</td>\n",
       "      <td>134</td>\n",
       "      <td>138</td>\n",
       "      <td>144</td>\n",
       "      <td>149</td>\n",
       "      <td>...</td>\n",
       "      <td>116</td>\n",
       "      <td>107</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>103</td>\n",
       "      <td>104</td>\n",
       "      <td>95</td>\n",
       "      <td>129</td>\n",
       "      <td>231</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>63</td>\n",
       "      <td>67</td>\n",
       "      <td>71</td>\n",
       "      <td>77</td>\n",
       "      <td>84</td>\n",
       "      <td>90</td>\n",
       "      <td>97</td>\n",
       "      <td>104</td>\n",
       "      <td>110</td>\n",
       "      <td>114</td>\n",
       "      <td>...</td>\n",
       "      <td>182</td>\n",
       "      <td>187</td>\n",
       "      <td>189</td>\n",
       "      <td>193</td>\n",
       "      <td>195</td>\n",
       "      <td>196</td>\n",
       "      <td>196</td>\n",
       "      <td>197</td>\n",
       "      <td>198</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22424</th>\n",
       "      <td>143</td>\n",
       "      <td>145</td>\n",
       "      <td>147</td>\n",
       "      <td>154</td>\n",
       "      <td>159</td>\n",
       "      <td>163</td>\n",
       "      <td>167</td>\n",
       "      <td>171</td>\n",
       "      <td>173</td>\n",
       "      <td>176</td>\n",
       "      <td>...</td>\n",
       "      <td>90</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>75</td>\n",
       "      <td>74</td>\n",
       "      <td>71</td>\n",
       "      <td>66</td>\n",
       "      <td>60</td>\n",
       "      <td>57</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22980</th>\n",
       "      <td>179</td>\n",
       "      <td>180</td>\n",
       "      <td>181</td>\n",
       "      <td>182</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>184</td>\n",
       "      <td>184</td>\n",
       "      <td>185</td>\n",
       "      <td>186</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>51</td>\n",
       "      <td>45</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>96</td>\n",
       "      <td>93</td>\n",
       "      <td>72</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  pixel9  \\\n",
       "17935     156     160     163     164     165     167     169     172     173   \n",
       "8315       87      92     105     115     125     132     134     138     144   \n",
       "3634       63      67      71      77      84      90      97     104     110   \n",
       "22424     143     145     147     154     159     163     167     171     173   \n",
       "22980     179     180     181     182     184     185     184     184     185   \n",
       "\n",
       "       pixel10    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "17935      172    ...           36        34        39        49        59   \n",
       "8315       149    ...          116       107       103       103       103   \n",
       "3634       114    ...          182       187       189       193       195   \n",
       "22424      176    ...           90        80        78        75        74   \n",
       "22980      186    ...           92        51        45        62        69   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "17935        73        70        89        72        33  \n",
       "8315        104        95       129       231       208  \n",
       "3634        196       196       197       198       198  \n",
       "22424        71        66        60        57        56  \n",
       "22980        80        96        93        72        54  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17935     3\n",
       "8315      3\n",
       "3634      5\n",
       "22424    15\n",
       "22980     1\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b) Preprocesamiento de los datos\n",
    "\n",
    "A continuacion ...\n",
    "\n",
    "Siendo que los datos son pixeles en escala de grises con valores de 1 a 255, se dividirá cada celda por 255 para que estén en un rango $[0,1]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = x_tr / 255\n",
    "X_val = x_v / 255 \n",
    "X_test = x_t / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "      <td>21964.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.570775</td>\n",
       "      <td>0.583081</td>\n",
       "      <td>0.593843</td>\n",
       "      <td>0.602733</td>\n",
       "      <td>0.613184</td>\n",
       "      <td>0.621862</td>\n",
       "      <td>0.629871</td>\n",
       "      <td>0.637137</td>\n",
       "      <td>0.643469</td>\n",
       "      <td>0.649665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.552954</td>\n",
       "      <td>0.578762</td>\n",
       "      <td>0.601246</td>\n",
       "      <td>0.623908</td>\n",
       "      <td>0.635747</td>\n",
       "      <td>0.638884</td>\n",
       "      <td>0.639750</td>\n",
       "      <td>0.636200</td>\n",
       "      <td>0.633063</td>\n",
       "      <td>0.628050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.162206</td>\n",
       "      <td>0.156203</td>\n",
       "      <td>0.152749</td>\n",
       "      <td>0.151216</td>\n",
       "      <td>0.145313</td>\n",
       "      <td>0.141487</td>\n",
       "      <td>0.137126</td>\n",
       "      <td>0.131865</td>\n",
       "      <td>0.127826</td>\n",
       "      <td>0.122365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250195</td>\n",
       "      <td>0.256740</td>\n",
       "      <td>0.252644</td>\n",
       "      <td>0.249881</td>\n",
       "      <td>0.249645</td>\n",
       "      <td>0.248626</td>\n",
       "      <td>0.248861</td>\n",
       "      <td>0.247994</td>\n",
       "      <td>0.249241</td>\n",
       "      <td>0.252143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.474510</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.521569</td>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.572549</td>\n",
       "      <td>0.580392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.439216</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.501961</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.498039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.619608</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.643137</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.650980</td>\n",
       "      <td>0.658824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.560784</td>\n",
       "      <td>0.635294</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.713725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.682353</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.792157</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pixel1        pixel2        pixel3        pixel4        pixel5  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       0.570775      0.583081      0.593843      0.602733      0.613184   \n",
       "std        0.162206      0.156203      0.152749      0.151216      0.145313   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.474510      0.494118      0.509804      0.521569      0.537255   \n",
       "50%        0.588235      0.600000      0.611765      0.619608      0.627451   \n",
       "75%        0.682353      0.690196      0.698039      0.705882      0.709804   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             pixel6        pixel7        pixel8        pixel9       pixel10  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       0.621862      0.629871      0.637137      0.643469      0.649665   \n",
       "std        0.141487      0.137126      0.131865      0.127826      0.122365   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.549020      0.556863      0.568627      0.572549      0.580392   \n",
       "50%        0.635294      0.643137      0.647059      0.650980      0.658824   \n",
       "75%        0.713725      0.717647      0.721569      0.725490      0.729412   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           ...           pixel775      pixel776      pixel777      pixel778  \\\n",
       "count      ...       21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       ...           0.552954      0.578762      0.601246      0.623908   \n",
       "std        ...           0.250195      0.256740      0.252644      0.249881   \n",
       "min        ...           0.000000      0.000000      0.000000      0.000000   \n",
       "25%        ...           0.360784      0.376471      0.403922      0.439216   \n",
       "50%        ...           0.560784      0.635294      0.674510      0.705882   \n",
       "75%        ...           0.768627      0.792157      0.803922      0.811765   \n",
       "max        ...           1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  21964.000000  21964.000000  21964.000000  21964.000000  21964.000000   \n",
       "mean       0.635747      0.638884      0.639750      0.636200      0.633063   \n",
       "std        0.249645      0.248626      0.248861      0.247994      0.249241   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.470588      0.494118      0.505882      0.501961      0.505882   \n",
       "50%        0.717647      0.721569      0.721569      0.717647      0.713725   \n",
       "75%        0.815686      0.811765      0.811765      0.807843      0.803922   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "           pixel784  \n",
       "count  21964.000000  \n",
       "mean       0.628050  \n",
       "std        0.252143  \n",
       "min        0.000000  \n",
       "25%        0.498039  \n",
       "50%        0.713725  \n",
       "75%        0.800000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17935</th>\n",
       "      <td>0.611765</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.643137</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.654902</td>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>0.678431</td>\n",
       "      <td>0.674510</td>\n",
       "      <td>...</td>\n",
       "      <td>0.141176</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.152941</td>\n",
       "      <td>0.192157</td>\n",
       "      <td>0.231373</td>\n",
       "      <td>0.286275</td>\n",
       "      <td>0.274510</td>\n",
       "      <td>0.349020</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.129412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8315</th>\n",
       "      <td>0.341176</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.517647</td>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.541176</td>\n",
       "      <td>0.564706</td>\n",
       "      <td>0.584314</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454902</td>\n",
       "      <td>0.419608</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.403922</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.815686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>0.247059</td>\n",
       "      <td>0.262745</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.301961</td>\n",
       "      <td>0.329412</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.407843</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.447059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.741176</td>\n",
       "      <td>0.756863</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.776471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22424</th>\n",
       "      <td>0.560784</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.603922</td>\n",
       "      <td>0.623529</td>\n",
       "      <td>0.639216</td>\n",
       "      <td>0.654902</td>\n",
       "      <td>0.670588</td>\n",
       "      <td>0.678431</td>\n",
       "      <td>0.690196</td>\n",
       "      <td>...</td>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.290196</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>0.258824</td>\n",
       "      <td>0.235294</td>\n",
       "      <td>0.223529</td>\n",
       "      <td>0.219608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22980</th>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.713725</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.721569</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.729412</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360784</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.243137</td>\n",
       "      <td>0.270588</td>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.376471</td>\n",
       "      <td>0.364706</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.211765</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         pixel1    pixel2    pixel3    pixel4    pixel5    pixel6    pixel7  \\\n",
       "17935  0.611765  0.627451  0.639216  0.643137  0.647059  0.654902  0.662745   \n",
       "8315   0.341176  0.360784  0.411765  0.450980  0.490196  0.517647  0.525490   \n",
       "3634   0.247059  0.262745  0.278431  0.301961  0.329412  0.352941  0.380392   \n",
       "22424  0.560784  0.568627  0.576471  0.603922  0.623529  0.639216  0.654902   \n",
       "22980  0.701961  0.705882  0.709804  0.713725  0.721569  0.725490  0.721569   \n",
       "\n",
       "         pixel8    pixel9   pixel10    ...     pixel775  pixel776  pixel777  \\\n",
       "17935  0.674510  0.678431  0.674510    ...     0.141176  0.133333  0.152941   \n",
       "8315   0.541176  0.564706  0.584314    ...     0.454902  0.419608  0.403922   \n",
       "3634   0.407843  0.431373  0.447059    ...     0.713725  0.733333  0.741176   \n",
       "22424  0.670588  0.678431  0.690196    ...     0.352941  0.313725  0.305882   \n",
       "22980  0.721569  0.725490  0.729412    ...     0.360784  0.200000  0.176471   \n",
       "\n",
       "       pixel778  pixel779  pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "17935  0.192157  0.231373  0.286275  0.274510  0.349020  0.282353  0.129412  \n",
       "8315   0.403922  0.403922  0.407843  0.372549  0.505882  0.905882  0.815686  \n",
       "3634   0.756863  0.764706  0.768627  0.768627  0.772549  0.776471  0.776471  \n",
       "22424  0.294118  0.290196  0.278431  0.258824  0.235294  0.223529  0.219608  \n",
       "22980  0.243137  0.270588  0.313725  0.376471  0.364706  0.282353  0.211765  \n",
       "\n",
       "[5 rows x 784 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAEICAYAAAAeBBZSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngZMkSZKknvj/SzzpYNoUhdEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbe516d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pylab import hist, show\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 4\n",
    "\n",
    "plt.title(\"Histograma de valores de pixel1 en conj. de entrenamiento\")\n",
    "plt.hist(X_train['pixel1'],bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "df_train_std = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
    "df_val_std = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "df_test_std = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "      <td>2.196400e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.896758e-16</td>\n",
       "      <td>-4.094996e-16</td>\n",
       "      <td>7.339280e-16</td>\n",
       "      <td>-2.957326e-16</td>\n",
       "      <td>5.011875e-16</td>\n",
       "      <td>5.391941e-16</td>\n",
       "      <td>1.104183e-15</td>\n",
       "      <td>3.579110e-16</td>\n",
       "      <td>5.001867e-16</td>\n",
       "      <td>-1.664728e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.101909e-19</td>\n",
       "      <td>3.118471e-16</td>\n",
       "      <td>-7.603213e-17</td>\n",
       "      <td>1.623999e-16</td>\n",
       "      <td>-2.813784e-16</td>\n",
       "      <td>-3.342044e-16</td>\n",
       "      <td>2.677191e-16</td>\n",
       "      <td>-3.691765e-16</td>\n",
       "      <td>7.389879e-17</td>\n",
       "      <td>3.195243e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "      <td>1.000023e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-3.518897e+00</td>\n",
       "      <td>-3.732917e+00</td>\n",
       "      <td>-3.887801e+00</td>\n",
       "      <td>-3.985997e+00</td>\n",
       "      <td>-4.219843e+00</td>\n",
       "      <td>-4.395279e+00</td>\n",
       "      <td>-4.593470e+00</td>\n",
       "      <td>-4.831839e+00</td>\n",
       "      <td>-5.034065e+00</td>\n",
       "      <td>-5.309335e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.210142e+00</td>\n",
       "      <td>-2.254328e+00</td>\n",
       "      <td>-2.379864e+00</td>\n",
       "      <td>-2.496877e+00</td>\n",
       "      <td>-2.546666e+00</td>\n",
       "      <td>-2.569723e+00</td>\n",
       "      <td>-2.570775e+00</td>\n",
       "      <td>-2.565441e+00</td>\n",
       "      <td>-2.540024e+00</td>\n",
       "      <td>-2.490904e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-5.934857e-01</td>\n",
       "      <td>-5.695477e-01</td>\n",
       "      <td>-5.501925e-01</td>\n",
       "      <td>-5.367587e-01</td>\n",
       "      <td>-5.225317e-01</td>\n",
       "      <td>-5.148439e-01</td>\n",
       "      <td>-5.324284e-01</td>\n",
       "      <td>-5.195545e-01</td>\n",
       "      <td>-5.548273e-01</td>\n",
       "      <td>-5.661231e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.680979e-01</td>\n",
       "      <td>-7.879426e-01</td>\n",
       "      <td>-7.810534e-01</td>\n",
       "      <td>-7.391390e-01</td>\n",
       "      <td>-6.615902e-01</td>\n",
       "      <td>-5.822817e-01</td>\n",
       "      <td>-5.379347e-01</td>\n",
       "      <td>-5.413118e-01</td>\n",
       "      <td>-5.102835e-01</td>\n",
       "      <td>-5.156336e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.076460e-01</td>\n",
       "      <td>1.083172e-01</td>\n",
       "      <td>1.173292e-01</td>\n",
       "      <td>1.115944e-01</td>\n",
       "      <td>9.818480e-02</td>\n",
       "      <td>9.493875e-02</td>\n",
       "      <td>9.674716e-02</td>\n",
       "      <td>7.524339e-02</td>\n",
       "      <td>5.876693e-02</td>\n",
       "      <td>7.485137e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>3.129597e-02</td>\n",
       "      <td>2.201973e-01</td>\n",
       "      <td>2.899948e-01</td>\n",
       "      <td>3.280588e-01</td>\n",
       "      <td>3.280744e-01</td>\n",
       "      <td>3.325725e-01</td>\n",
       "      <td>3.287801e-01</td>\n",
       "      <td>3.284313e-01</td>\n",
       "      <td>3.236410e-01</td>\n",
       "      <td>3.397985e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.878930e-01</td>\n",
       "      <td>6.857577e-01</td>\n",
       "      <td>6.821553e-01</td>\n",
       "      <td>6.821452e-01</td>\n",
       "      <td>6.649259e-01</td>\n",
       "      <td>6.492866e-01</td>\n",
       "      <td>6.401260e-01</td>\n",
       "      <td>6.403014e-01</td>\n",
       "      <td>6.416814e-01</td>\n",
       "      <td>6.517284e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>8.620386e-01</td>\n",
       "      <td>8.311912e-01</td>\n",
       "      <td>8.022352e-01</td>\n",
       "      <td>7.517992e-01</td>\n",
       "      <td>7.207985e-01</td>\n",
       "      <td>6.953594e-01</td>\n",
       "      <td>6.912244e-01</td>\n",
       "      <td>6.921420e-01</td>\n",
       "      <td>6.855328e-01</td>\n",
       "      <td>6.819713e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.646226e+00</td>\n",
       "      <td>2.669140e+00</td>\n",
       "      <td>2.659046e+00</td>\n",
       "      <td>2.627204e+00</td>\n",
       "      <td>2.662014e+00</td>\n",
       "      <td>2.672656e+00</td>\n",
       "      <td>2.699246e+00</td>\n",
       "      <td>2.751834e+00</td>\n",
       "      <td>2.789261e+00</td>\n",
       "      <td>2.863091e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.786828e+00</td>\n",
       "      <td>1.640758e+00</td>\n",
       "      <td>1.578357e+00</td>\n",
       "      <td>1.505115e+00</td>\n",
       "      <td>1.459120e+00</td>\n",
       "      <td>1.452480e+00</td>\n",
       "      <td>1.447630e+00</td>\n",
       "      <td>1.467004e+00</td>\n",
       "      <td>1.472254e+00</td>\n",
       "      <td>1.475190e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 784 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             pixel1        pixel2        pixel3        pixel4        pixel5  \\\n",
       "count  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04   \n",
       "mean   5.896758e-16 -4.094996e-16  7.339280e-16 -2.957326e-16  5.011875e-16   \n",
       "std    1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00   \n",
       "min   -3.518897e+00 -3.732917e+00 -3.887801e+00 -3.985997e+00 -4.219843e+00   \n",
       "25%   -5.934857e-01 -5.695477e-01 -5.501925e-01 -5.367587e-01 -5.225317e-01   \n",
       "50%    1.076460e-01  1.083172e-01  1.173292e-01  1.115944e-01  9.818480e-02   \n",
       "75%    6.878930e-01  6.857577e-01  6.821553e-01  6.821452e-01  6.649259e-01   \n",
       "max    2.646226e+00  2.669140e+00  2.659046e+00  2.627204e+00  2.662014e+00   \n",
       "\n",
       "             pixel6        pixel7        pixel8        pixel9       pixel10  \\\n",
       "count  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04   \n",
       "mean   5.391941e-16  1.104183e-15  3.579110e-16  5.001867e-16 -1.664728e-16   \n",
       "std    1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00   \n",
       "min   -4.395279e+00 -4.593470e+00 -4.831839e+00 -5.034065e+00 -5.309335e+00   \n",
       "25%   -5.148439e-01 -5.324284e-01 -5.195545e-01 -5.548273e-01 -5.661231e-01   \n",
       "50%    9.493875e-02  9.674716e-02  7.524339e-02  5.876693e-02  7.485137e-02   \n",
       "75%    6.492866e-01  6.401260e-01  6.403014e-01  6.416814e-01  6.517284e-01   \n",
       "max    2.672656e+00  2.699246e+00  2.751834e+00  2.789261e+00  2.863091e+00   \n",
       "\n",
       "           ...           pixel775      pixel776      pixel777      pixel778  \\\n",
       "count      ...       2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04   \n",
       "mean       ...      -7.101909e-19  3.118471e-16 -7.603213e-17  1.623999e-16   \n",
       "std        ...       1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00   \n",
       "min        ...      -2.210142e+00 -2.254328e+00 -2.379864e+00 -2.496877e+00   \n",
       "25%        ...      -7.680979e-01 -7.879426e-01 -7.810534e-01 -7.391390e-01   \n",
       "50%        ...       3.129597e-02  2.201973e-01  2.899948e-01  3.280588e-01   \n",
       "75%        ...       8.620386e-01  8.311912e-01  8.022352e-01  7.517992e-01   \n",
       "max        ...       1.786828e+00  1.640758e+00  1.578357e+00  1.505115e+00   \n",
       "\n",
       "           pixel779      pixel780      pixel781      pixel782      pixel783  \\\n",
       "count  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04  2.196400e+04   \n",
       "mean  -2.813784e-16 -3.342044e-16  2.677191e-16 -3.691765e-16  7.389879e-17   \n",
       "std    1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00  1.000023e+00   \n",
       "min   -2.546666e+00 -2.569723e+00 -2.570775e+00 -2.565441e+00 -2.540024e+00   \n",
       "25%   -6.615902e-01 -5.822817e-01 -5.379347e-01 -5.413118e-01 -5.102835e-01   \n",
       "50%    3.280744e-01  3.325725e-01  3.287801e-01  3.284313e-01  3.236410e-01   \n",
       "75%    7.207985e-01  6.953594e-01  6.912244e-01  6.921420e-01  6.855328e-01   \n",
       "max    1.459120e+00  1.452480e+00  1.447630e+00  1.467004e+00  1.472254e+00   \n",
       "\n",
       "           pixel784  \n",
       "count  2.196400e+04  \n",
       "mean   3.195243e-17  \n",
       "std    1.000023e+00  \n",
       "min   -2.490904e+00  \n",
       "25%   -5.156336e-01  \n",
       "50%    3.397985e-01  \n",
       "75%    6.819713e-01  \n",
       "max    1.475190e+00  \n",
       "\n",
       "[8 rows x 784 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_std.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25270775, -1.41550224, -1.99574919, ...,  0.08346906,\n",
       "        0.44612341, -0.73854746])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_std['pixel1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3AAAAEICAYAAAAeBBZSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAidJkiRJPfH/AZcdMSydOPYiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xbd10a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Histograma de valores de pixel1 en conj. de entrenamiento\")\n",
    "plt.hist(df_train_std['pixel1'],bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Boti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Boti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Boti\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\Boti\\Anaconda3\\lib\\site-packages\\keras\\models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 15.4297 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 2/100\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 3/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 4/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4877 - acc: 0.03 - 2s 85us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 5/100\n",
      "21964/21964 [==============================] - 2s 73us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 6/100\n",
      "21964/21964 [==============================] - 3s 114us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 7/100\n",
      "21964/21964 [==============================] - 2s 104us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 8/100\n",
      "21964/21964 [==============================] - 2s 81us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 9/100\n",
      "21964/21964 [==============================] - 2s 86us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 10/100\n",
      "21964/21964 [==============================] - 3s 114us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 11/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4908 - acc: 0.03 - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 12/100\n",
      "21964/21964 [==============================] - 3s 115us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 13/100\n",
      "21964/21964 [==============================] - 2s 99us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 14/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4937 - acc: 0.03 - 2s 90us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 15/100\n",
      "21964/21964 [==============================] - 3s 135us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 16/100\n",
      "21964/21964 [==============================] - 2s 107us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 17/100\n",
      "21964/21964 [==============================] - 3s 119us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 18/100\n",
      "21964/21964 [==============================] - 2s 98us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 19/100\n",
      "21964/21964 [==============================] - 2s 105us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 20/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 21/100\n",
      "21964/21964 [==============================] - 2s 93us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 22/100\n",
      "21964/21964 [==============================] - 1s 53us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 23/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 24/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 25/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 26/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 27/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 28/100\n",
      "21964/21964 [==============================] - 1s 44us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 29/100\n",
      "21964/21964 [==============================] - 2s 104us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437oss:\n",
      "Epoch 30/100\n",
      "21964/21964 [==============================] - 2s 99us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 31/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 32/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 33/100\n",
      "21964/21964 [==============================] - 3s 129us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 34/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 35/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 36/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 37/100\n",
      "21964/21964 [==============================] - 2s 80us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 38/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 39/100\n",
      "21964/21964 [==============================] - 2s 103us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 40/100\n",
      "21964/21964 [==============================] - 2s 112us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 41/100\n",
      "21964/21964 [==============================] - 2s 111us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 42/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 43/100\n",
      "21964/21964 [==============================] - 3s 116us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 44/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 45/100\n",
      "21964/21964 [==============================] - 2s 113us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 46/100\n",
      "21964/21964 [==============================] - 3s 133us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 47/100\n",
      "21964/21964 [==============================] - 2s 112us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 48/100\n",
      "21964/21964 [==============================] - 4s 165us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 49/100\n",
      "21964/21964 [==============================] - 3s 140us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 50/100\n",
      "21964/21964 [==============================] - 3s 128us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 51/100\n",
      "21964/21964 [==============================] - 3s 115us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 52/100\n",
      "21964/21964 [==============================] - 3s 143us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 53/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 54/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 55/100\n",
      "21964/21964 [==============================] - 2s 70us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 56/100\n",
      "21964/21964 [==============================] - 3s 116us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 57/100\n",
      "21964/21964 [==============================] - 1s 56us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "21964/21964 [==============================] - 2s 68us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 59/100\n",
      "21964/21964 [==============================] - 2s 110us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 60/100\n",
      "21964/21964 [==============================] - 3s 119us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 61/100\n",
      "21964/21964 [==============================] - 2s 86us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 62/100\n",
      "21964/21964 [==============================] - 3s 118us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 63/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 64/100\n",
      "21964/21964 [==============================] - 2s 74us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 65/100\n",
      "21964/21964 [==============================] - 2s 77us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 66/100\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 67/100\n",
      "21964/21964 [==============================] - 3s 130us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 68/100\n",
      "21964/21964 [==============================] - 3s 127us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 69/100\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 70/100\n",
      "21964/21964 [==============================] - 3s 139us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 71/100\n",
      "21964/21964 [==============================] - 2s 82us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 72/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 73/100\n",
      "21964/21964 [==============================] - 2s 73us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 74/100\n",
      "21964/21964 [==============================] - 2s 110us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 75/100\n",
      "21964/21964 [==============================] - 1s 61us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 76/100\n",
      "21964/21964 [==============================] - 2s 88us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 77/100\n",
      "21964/21964 [==============================] - 3s 133us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 78/100\n",
      "21964/21964 [==============================] - 2s 105us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 79/100\n",
      "21964/21964 [==============================] - 3s 133us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 80/100\n",
      "21964/21964 [==============================] - 2s 93us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 81/100\n",
      "21964/21964 [==============================] - 2s 112us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 82/100\n",
      "21964/21964 [==============================] - 3s 141us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.04375.49\n",
      "Epoch 83/100\n",
      "21964/21964 [==============================] - 1s 67us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 84/100\n",
      "21964/21964 [==============================] - 2s 74us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 85/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 86/100\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 87/100\n",
      "21964/21964 [==============================] - 3s 139us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 88/100\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 89/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 90/100\n",
      "21964/21964 [==============================] - 2s 86us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 91/100\n",
      "21964/21964 [==============================] - 3s 126us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 92/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 93/100\n",
      "21964/21964 [==============================] - 2s 113us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 94/100\n",
      "21964/21964 [==============================] - 2s 81us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 95/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 96/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4922 - acc: 0.0388 ETA: 0s - loss: 15.4927 - - 3s 119us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 97/100\n",
      "21964/21964 [==============================] - 1s 64us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 98/100\n",
      "21964/21964 [==============================] - 1s 57us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 99/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 15.4892 - acc: 0.0390 ETA:  - 2s 79us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n",
      "Epoch 100/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 15.4943 - acc: 0.0387 - val_loss: 15.4136 - val_acc: 0.0437\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28abfe48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Modelo secuencial (feed forward)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr.values, to_categorical(y_tr), nb_epoch=100, batch_size=128, verbose=1, validation_data=(x_v.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21964 samples, validate on 5491 samples\n",
      "Epoch 1/100\n",
      "21964/21964 [==============================] - 2s 91us/step - loss: 3.1924 - acc: 0.0982 - val_loss: 3.1207 - val_acc: 0.1018\n",
      "Epoch 2/100\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 2.8818 - acc: 0.1269 - val_loss: 2.6251 - val_acc: 0.1515\n",
      "Epoch 3/100\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 2.2955 - acc: 0.2632 - val_loss: 1.9061 - val_acc: 0.3661\n",
      "Epoch 4/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 1.6256 - acc: 0.4547 - val_loss: 1.4097 - val_acc: 0.5265\n",
      "Epoch 5/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 1.1973 - acc: 0.5930 - val_loss: 1.0383 - val_acc: 0.6431\n",
      "Epoch 6/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 0.8562 - acc: 0.7184 - val_loss: 0.7440 - val_acc: 0.7483loss: 0.8873 - ac\n",
      "Epoch 7/100\n",
      "21964/21964 [==============================] - 2s 111us/step - loss: 0.5947 - acc: 0.8117 - val_loss: 0.5115 - val_acc: 0.8510\n",
      "Epoch 8/100\n",
      "21964/21964 [==============================] - 1s 59us/step - loss: 0.4057 - acc: 0.8829 - val_loss: 0.3555 - val_acc: 0.9075\n",
      "Epoch 9/100\n",
      "21964/21964 [==============================] - 2s 73us/step - loss: 0.2702 - acc: 0.9358 - val_loss: 0.2247 - val_acc: 0.9556\n",
      "Epoch 10/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 0.1774 - acc: 0.9668 - val_loss: 0.1538 - val_acc: 0.9752\n",
      "Epoch 11/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 0.1195 - acc: 0.984 - 1s 49us/step - loss: 0.1183 - acc: 0.9842 - val_loss: 0.1067 - val_acc: 0.9854\n",
      "Epoch 12/100\n",
      "21964/21964 [==============================] - 2s 105us/step - loss: 0.0817 - acc: 0.9931 - val_loss: 0.0735 - val_acc: 0.9962\n",
      "Epoch 13/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 0.0574 - acc: 0.997 - 1s 49us/step - loss: 0.0574 - acc: 0.9970 - val_loss: 0.0570 - val_acc: 0.9969\n",
      "Epoch 14/100\n",
      "21964/21964 [==============================] - ETA: 0s - loss: 0.0427 - acc: 0.999 - 2s 92us/step - loss: 0.0427 - acc: 0.9992 - val_loss: 0.0425 - val_acc: 0.9982\n",
      "Epoch 15/100\n",
      "21964/21964 [==============================] - 2s 92us/step - loss: 0.0333 - acc: 0.9994 - val_loss: 0.0338 - val_acc: 0.9996\n",
      "Epoch 16/100\n",
      "21964/21964 [==============================] - 2s 79us/step - loss: 0.0266 - acc: 0.9995 - val_loss: 0.0274 - val_acc: 0.9989\n",
      "Epoch 17/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 0.0220 - acc: 0.9996 - val_loss: 0.0229 - val_acc: 0.9996 1s - loss: 0.02 - ETA: 0s - loss: 0.0220 - acc: 0.\n",
      "Epoch 18/100\n",
      "21964/21964 [==============================] - 2s 110us/step - loss: 0.0184 - acc: 0.9999 - val_loss: 0.0198 - val_acc: 0.9995\n",
      "Epoch 19/100\n",
      "21964/21964 [==============================] - 2s 109us/step - loss: 0.0157 - acc: 0.9999 - val_loss: 0.0174 - val_acc: 0.9996\n",
      "Epoch 20/100\n",
      "21964/21964 [==============================] - 1s 66us/step - loss: 0.0137 - acc: 0.9999 - val_loss: 0.0153 - val_acc: 0.9995\n",
      "Epoch 21/100\n",
      "21964/21964 [==============================] - 2s 92us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.0137 - val_acc: 0.9995\n",
      "Epoch 22/100\n",
      "21964/21964 [==============================] - 2s 72us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 0.0123 - val_acc: 0.9996\n",
      "Epoch 23/100\n",
      "21964/21964 [==============================] - 1s 47us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.0113 - val_acc: 0.9996\n",
      "Epoch 24/100\n",
      "21964/21964 [==============================] - 2s 104us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.0105 - val_acc: 0.9995c: 1.0\n",
      "Epoch 25/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0096 - val_acc: 0.9996\n",
      "Epoch 26/100\n",
      "21964/21964 [==============================] - 2s 87us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0088 - val_acc: 0.9998\n",
      "Epoch 27/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.0083 - val_acc: 0.9996\n",
      "Epoch 28/100\n",
      "21964/21964 [==============================] - 2s 98us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0077 - val_acc: 0.9996\n",
      "Epoch 29/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0073 - val_acc: 0.9998\n",
      "Epoch 30/100\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.0068 - val_acc: 0.9998\n",
      "Epoch 31/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.0065 - val_acc: 0.9998\n",
      "Epoch 32/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.0061 - val_acc: 0.9998\n",
      "Epoch 33/100\n",
      "21964/21964 [==============================] - 2s 111us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0058 - val_acc: 0.9998\n",
      "Epoch 34/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 0.0055 - val_acc: 0.9998\n",
      "Epoch 35/100\n",
      "21964/21964 [==============================] - 2s 101us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0053 - val_acc: 0.9998s: 0.0041 \n",
      "Epoch 36/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 0.0051 - val_acc: 0.9998\n",
      "Epoch 37/100\n",
      "21964/21964 [==============================] - 2s 75us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0049 - val_acc: 0.9998\n",
      "Epoch 38/100\n",
      "21964/21964 [==============================] - 1s 48us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 0.0046 - val_acc: 0.9998\n",
      "Epoch 39/100\n",
      "21964/21964 [==============================] - 2s 70us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0045 - val_acc: 0.9998\n",
      "Epoch 40/100\n",
      "21964/21964 [==============================] - 2s 85us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0043 - val_acc: 0.9998\n",
      "Epoch 41/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0042 - val_acc: 0.9998\n",
      "Epoch 42/100\n",
      "21964/21964 [==============================] - 2s 79us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0040 - val_acc: 0.9998\n",
      "Epoch 43/100\n",
      "21964/21964 [==============================] - 3s 122us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9998\n",
      "Epoch 44/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0038 - val_acc: 0.9998\n",
      "Epoch 45/100\n",
      "21964/21964 [==============================] - 2s 105us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.0036 - val_acc: 0.9998\n",
      "Epoch 46/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 0.0025 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9998\n",
      "Epoch 47/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0035 - val_acc: 0.9998\n",
      "Epoch 48/100\n",
      "21964/21964 [==============================] - 2s 102us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9998\n",
      "Epoch 49/100\n",
      "21964/21964 [==============================] - 1s 50us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.0033 - val_acc: 0.9998\n",
      "Epoch 50/100\n",
      "21964/21964 [==============================] - 2s 87us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0032 - val_acc: 0.9998\n",
      "Epoch 51/100\n",
      "21964/21964 [==============================] - 2s 93us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0031 - val_acc: 0.9998\n",
      "Epoch 52/100\n",
      "21964/21964 [==============================] - 3s 154us/step - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0030 - val_acc: 0.9998\n",
      "Epoch 53/100\n",
      "21964/21964 [==============================] - 3s 139us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.999800 - ETA: 1s - loss: 0.0020 - acc: 1.0 - ETA: 1s - loss: 0.0020 - ETA: 0s - loss: 0.0020 - acc: 1.0\n",
      "Epoch 54/100\n",
      "21964/21964 [==============================] - 2s 83us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0029 - val_acc: 0.9998\n",
      "Epoch 55/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0028 - val_acc: 0.9998\n",
      "Epoch 56/100\n",
      "21964/21964 [==============================] - 2s 106us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0027 - val_acc: 0.9998\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21964/21964 [==============================] - 2s 94us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9998\n",
      "Epoch 58/100\n",
      "21964/21964 [==============================] - 3s 130us/step - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9998 acc\n",
      "Epoch 59/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0026 - val_acc: 0.9998\n",
      "Epoch 60/100\n",
      "21964/21964 [==============================] - 3s 127us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0025 - val_acc: 0.9998\n",
      "Epoch 61/100\n",
      "21964/21964 [==============================] - 1s 67us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9998\n",
      "Epoch 62/100\n",
      "21964/21964 [==============================] - 3s 139us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0024 - val_acc: 0.9998\n",
      "Epoch 63/100\n",
      "21964/21964 [==============================] - 2s 90us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9998\n",
      "Epoch 64/100\n",
      "21964/21964 [==============================] - 3s 135us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9998\n",
      "Epoch 65/100\n",
      "21964/21964 [==============================] - 3s 147us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0023 - val_acc: 0.9998\n",
      "Epoch 66/100\n",
      "21964/21964 [==============================] - 3s 122us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9998\n",
      "Epoch 67/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9998\n",
      "Epoch 68/100\n",
      "21964/21964 [==============================] - 2s 108us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0022 - val_acc: 0.9998\n",
      "Epoch 69/100\n",
      "21964/21964 [==============================] - 3s 117us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9998\n",
      "Epoch 70/100\n",
      "21964/21964 [==============================] - 2s 107us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0021 - val_acc: 0.9998s - loss: 0.0013 - ac\n",
      "Epoch 71/100\n",
      "21964/21964 [==============================] - 2s 92us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Epoch 72/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Epoch 73/100\n",
      "21964/21964 [==============================] - 3s 114us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Epoch 74/100\n",
      "21964/21964 [==============================] - 3s 120us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0020 - val_acc: 0.9998\n",
      "Epoch 75/100\n",
      "21964/21964 [==============================] - 1s 60us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9998\n",
      "Epoch 76/100\n",
      "21964/21964 [==============================] - 2s 107us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9998\n",
      "Epoch 77/100\n",
      "21964/21964 [==============================] - 2s 114us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0019 - val_acc: 0.9998\n",
      "Epoch 78/100\n",
      "21964/21964 [==============================] - 2s 83us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9998\n",
      "Epoch 79/100\n",
      "21964/21964 [==============================] - 2s 94us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9998\n",
      "Epoch 80/100\n",
      "21964/21964 [==============================] - 2s 82us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9998\n",
      "Epoch 81/100\n",
      "21964/21964 [==============================] - 3s 127us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0018 - val_acc: 0.9998- loss: 0.0010 - acc: 1. - ETA: 1s - loss: 0.0011 - acc: 1.00 - \n",
      "Epoch 82/100\n",
      "21964/21964 [==============================] - 1s 54us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 83/100\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 84/100\n",
      "21964/21964 [==============================] - 2s 73us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 85/100\n",
      "21964/21964 [==============================] - 2s 106us/step - loss: 9.9765e-04 - acc: 1.0000 - val_loss: 0.0017 - val_acc: 0.9998\n",
      "Epoch 86/100\n",
      "21964/21964 [==============================] - 1s 62us/step - loss: 9.8002e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 87/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 9.6427e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 88/100\n",
      "21964/21964 [==============================] - 2s 113us/step - loss: 9.4980e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998 9.5428e-04 - acc: 1. - ETA: 0s - loss: 9.5467e-04 - acc: 1.0\n",
      "Epoch 89/100\n",
      "21964/21964 [==============================] - 3s 119us/step - loss: 9.3369e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 90/100\n",
      "21964/21964 [==============================] - 2s 79us/step - loss: 9.2000e-04 - acc: 1.0000 - val_loss: 0.0016 - val_acc: 0.9998\n",
      "Epoch 91/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 9.0570e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998s - loss: 9.1405e-04 - - ETA: 0s - loss: 9.1211e-04 - \n",
      "Epoch 92/100\n",
      "21964/21964 [==============================] - 1s 55us/step - loss: 8.9155e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998\n",
      "Epoch 93/100\n",
      "21964/21964 [==============================] - 1s 49us/step - loss: 8.7889e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998\n",
      "Epoch 94/100\n",
      "21964/21964 [==============================] - 1s 51us/step - loss: 8.6558e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998\n",
      "Epoch 95/100\n",
      "21964/21964 [==============================] - 1s 58us/step - loss: 8.5277e-04 - acc: 1.0000 - val_loss: 0.0015 - val_acc: 0.9998\n",
      "Epoch 96/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 8.4051e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 97/100\n",
      "21964/21964 [==============================] - 1s 52us/step - loss: 8.2839e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 98/100\n",
      "21964/21964 [==============================] - 2s 80us/step - loss: 8.1706e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 99/100\n",
      "21964/21964 [==============================] - 2s 96us/step - loss: 8.0572e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n",
      "Epoch 100/100\n",
      "21964/21964 [==============================] - 2s 100us/step - loss: 7.9464e-04 - acc: 1.0000 - val_loss: 0.0014 - val_acc: 0.9998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x35261f60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# Modelo secuencial (feed forward)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(30, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(25, kernel_initializer='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(df_train_std.values, to_categorical(y_tr), epochs=100, batch_size=128, verbose=1, validation_data=(df_val_std.values,to_categorical(y_v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_3 to have shape (None, 25) but got array with shape (7172, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-eee308cb78d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mloss_and_metrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[0;32m    920\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    921\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 922\u001b[1;33m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    923\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    924\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[0;32m   1679\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1680\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1681\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1682\u001b[0m         \u001b[1;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1683\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[0;32m   1411\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1413\u001b[1;33m                                     exception_prefix='target')\n\u001b[0m\u001b[0;32m   1414\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[0;32m   1415\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    152\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                             str(array.shape))\n\u001b[0m\u001b[0;32m    155\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_3 to have shape (None, 25) but got array with shape (7172, 1)"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(x_t.values, y_t.values, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d) Matriz de Confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e) Clasificación mediante SVM no lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified: 'C:\\\\Users\\\\Boti\\\\Anaconda3\\\\lib\\\\site-packages\\\\numpy\\\\fft\\\\__init__.cp36-win_amd64.pyd'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e824e1f94ad7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_std\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mY_pred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mY_pred_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_val_std\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[1;31m# avoid flakes unused variable error\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfft\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpolynomial\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_isfile\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_is_mode_type\u001b[1;34m(path, mode)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC(verbose=10, C=0.1)\n",
    "model.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model.predict(df_train_std)\n",
    "Y_pred_val = model.predict(df_val_std)\n",
    "Y_pred_test = model.predict(df_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.score(df_val_std, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f) Clasificación mediante Árbol de Clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  1.0\n",
      "Score Val:  0.875978874522\n",
      "Score Test:  0.458170663692\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "\n",
    "model_tree_default = Tree(random_state=0) # Arbol por defecto\n",
    "model_tree_default.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model_tree_default.predict(df_train_std)\n",
    "Y_pred_val = model_tree_default.predict(df_val_std)\n",
    "Y_pred_test = model_tree_default.predict(df_test_std)\n",
    "\n",
    "print(\"Score Train: \", model_tree_default.score(df_train_std, y_tr))\n",
    "print(\"Score Val: \", model_tree_default.score(df_val_std, y_v))\n",
    "print(\"Score Test: \", model_tree_default.score(df_test_std, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=0,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tree_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 180 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   12.0s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   17.9s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   23.1s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   27.7s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   32.5s\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:   37.8s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:   42.5s\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed:   46.1s\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:   52.2s\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 234 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 257 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done 305 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 330 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done 360 out of 360 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores params: {'max_depth': 50, 'min_samples_leaf': 5, 'min_samples_split': 5}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_depth':[10, 20, 50, 200, None], 'min_samples_split': [5,10,15,20,50,100], 'min_samples_leaf': [5,10,15,20,50,100] }\n",
    "clf = GridSearchCV(Tree(random_state=0), parameters, verbose=10, n_jobs=4, cv=2)\n",
    "clf.fit(df_val_std, y_v)\n",
    "tree_model_cv = clf.best_estimator_\n",
    "print (\"Mejores params:\", clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 196 candidates, totalling 392 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   11.9s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   19.2s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   23.2s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   32.6s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:   43.5s\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:   50.2s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=4)]: Done 234 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done 257 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done 305 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=4)]: Done 330 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=4)]: Done 384 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done 392 out of 392 | elapsed:  3.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores params: {'max_depth': 35, 'min_samples_leaf': 2, 'min_samples_split': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'max_depth':[20, 35, 50, 60], 'min_samples_split': [2,3,4,5,6,7,8], 'min_samples_leaf': [2,3,4,5,6,7,8] }\n",
    "clf = GridSearchCV(Tree(random_state=0), parameters, verbose=10, n_jobs=4, cv=2)\n",
    "clf.fit(df_val_std, y_v)\n",
    "tree_model_cv = clf.best_estimator_\n",
    "print (\"Mejores params:\", clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.931342196321\n",
      "Score Val:  0.83081405937\n",
      "Score Test:  0.442136084774\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV1\n",
    "# Datos sin preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=50, min_samples_leaf=5, min_samples_split=5)\n",
    "model.fit(x_tr, y_tr)\n",
    "Y_pred_train = model.predict(x_tr)\n",
    "Y_pred_val = model.predict(x_v)\n",
    "Y_pred_test = model.predict(x_t)\n",
    "\n",
    "print(\"Score Train: \", model.score(x_tr, y_tr))\n",
    "print(\"Score Val: \", model.score(x_v, y_v))\n",
    "print(\"Score Test: \", model.score(x_t, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.931342196321\n",
      "Score Val:  0.83044982699\n",
      "Score Test:  0.442554378137\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV1\n",
    "# Datos con preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=50, min_samples_leaf=5, min_samples_split=5)\n",
    "model.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model.predict(df_train_std)\n",
    "Y_pred_val = model.predict(df_val_std)\n",
    "Y_pred_test = model.predict(df_test_std)\n",
    "\n",
    "print(\"Score Train: \", model.score(df_train_std, y_tr))\n",
    "print(\"Score Val: \", model.score(df_val_std, y_v))\n",
    "print(\"Score Test: \", model.score(df_test_std, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.966991440539\n",
      "Score Val:  0.853760699326\n",
      "Score Test:  0.439068600112\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV2\n",
    "# Datos sin preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=35, min_samples_leaf=2, min_samples_split=6)\n",
    "model.fit(x_tr, y_tr)\n",
    "Y_pred_train = model.predict(x_tr)\n",
    "Y_pred_val = model.predict(x_v)\n",
    "Y_pred_test = model.predict(x_t)\n",
    "\n",
    "print(\"Score Train: \", model.score(x_tr, y_tr))\n",
    "print(\"Score Val: \", model.score(x_v, y_v))\n",
    "print(\"Score Test: \", model.score(x_t, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.966991440539\n",
      "Score Val:  0.852850118376\n",
      "Score Test:  0.439486893475\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV2\n",
    "# Datos con preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=35, min_samples_leaf=2, min_samples_split=6)\n",
    "model.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model.predict(df_train_std)\n",
    "Y_pred_val = model.predict(df_val_std)\n",
    "Y_pred_test = model.predict(df_test_std)\n",
    "\n",
    "print(\"Score Train: \", model.score(df_train_std, y_tr))\n",
    "print(\"Score Val: \", model.score(df_val_std, y_v))\n",
    "print(\"Score Test: \", model.score(df_test_std, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.917273720634\n",
      "Score Val:  0.820797668913\n",
      "Score Test:  0.436001115449\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV3\n",
    "# Datos sin preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=20, min_samples_leaf=5, min_samples_split=5)\n",
    "model.fit(x_tr, y_tr)\n",
    "Y_pred_train = model.predict(x_tr)\n",
    "Y_pred_val = model.predict(x_v)\n",
    "Y_pred_test = model.predict(x_t)\n",
    "\n",
    "print(\"Score Train: \", model.score(x_tr, y_tr))\n",
    "print(\"Score Val: \", model.score(x_v, y_v))\n",
    "print(\"Score Test: \", model.score(x_t, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score Train:  0.917273720634\n",
      "Score Val:  0.820069204152\n",
      "Score Test:  0.436279977691\n"
     ]
    }
   ],
   "source": [
    "# Modelo GSCV3\n",
    "# Datos con preprocesamiento\n",
    "model = Tree(random_state=0, max_depth=20, min_samples_leaf=5, min_samples_split=5)\n",
    "model.fit(df_train_std, y_tr)\n",
    "Y_pred_train = model.predict(df_train_std)\n",
    "Y_pred_val = model.predict(df_val_std)\n",
    "Y_pred_test = model.predict(df_test_std)\n",
    "\n",
    "print(\"Score Train: \", model.score(df_train_std, y_tr))\n",
    "print(\"Score Val: \", model.score(df_val_std, y_v))\n",
    "print(\"Score Test: \", model.score(df_test_std, y_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
